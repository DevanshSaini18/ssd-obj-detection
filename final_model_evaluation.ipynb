{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DevanshSaini18/ssd-obj-detection/blob/main/final_model_evaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAFGTy-ycHV0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91054b36-24fd-4c4e-9192-a33d11898157"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Drive setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "276ZYFEEdyuH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "from torchvision import datasets\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "from google.colab.patches import cv2_imshow\n",
        "from torchvision.transforms import ToTensor\n",
        " \n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import pickle\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "from torchvision.io import ImageReadMode, read_image\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.image as mpimg\n",
        "import cv2\n",
        "import pickle\n",
        "import warnings\n",
        "import copy\n",
        "import pickle\n",
        "from google.colab.patches import cv2_imshow\n",
        "import random\n",
        "from sklearn.metrics import average_precision_score\n",
        "import math\n",
        "import torchvision\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnXdW1eFRe_x"
      },
      "source": [
        "## Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lnfyhSyFbFmn"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/SSD detection/images_2.pkl', 'rb') as f:\n",
        "    images = pickle.load(f)\n",
        "with open('/content/drive/MyDrive/SSD detection/labels_2.pkl', 'rb') as f:\n",
        "    labels = pickle.load(f)\n",
        "labels = [[label] for label in labels]\n",
        "with open('/content/drive/MyDrive/SSD detection/coco_person_fire_hydrant_image_3.pickle', 'rb') as f:\n",
        "    images_rest = pickle.load(f)\n",
        "    # images_rest = []\n",
        "for idx in range(len(images_rest)):\n",
        "  images_rest[idx] = np.array(images_rest[idx])\n",
        "with open('/content/drive/MyDrive/SSD detection/coco_person_fire_hydrant_class_labels_3.pickle', 'rb') as f:\n",
        "    labels_rest = pickle.load(f)\n",
        "    # labels_rest = []\n",
        "with open('/content/drive/MyDrive/SSD detection/coco_person_fire_hydrant_annotation_3.pickle', 'rb') as f:\n",
        "    annotations = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a_gkq3ZEbVAb"
      },
      "outputs": [],
      "source": [
        "random.Random(4).shuffle(images)\n",
        "random.Random(4).shuffle(labels)\n",
        "random.Random(5).shuffle(images_rest)\n",
        "random.Random(5).shuffle(labels_rest)\n",
        "random.Random(5).shuffle(annotations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxilYopfXT3E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea09a40e-64e8-47f3-fe5f-e599bd26cf76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "459\n",
            "459\n",
            "438\n",
            "438\n",
            "438\n"
          ]
        }
      ],
      "source": [
        "print(len(images))\n",
        "print(len(labels))\n",
        "print(len(images_rest))\n",
        "print(len(labels_rest))\n",
        "print(len(annotations))\n",
        "# print(len(images_door))\n",
        "# print(len(annotations_door))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z06Bh0F0Xqd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "113509c5-5857-4ece-e152-d5e72e0b7621"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[137, 91, 226, 217]], [[96, 109, 123, 142]], [[88, 176, 163, 281]], [[67, 1, 163, 138]], [[79, 2, 246, 260]]]\n",
            "[['fire hydrant', 'person', 'person'], ['person', 'person', 'fire hydrant'], ['person', 'person', 'fire hydrant'], ['person', 'person', 'fire hydrant', 'person'], ['fire hydrant', 'person', 'person', 'person']]\n",
            "[[[104, 123, 237, 313], [32, 19, 156, 283], [184, 34, 295, 276]], [[33, 47, 178, 305], [176, 62, 303, 315], [155, 52, 236, 319]], [[234, 24, 303, 278], [115, 50, 145, 85], [15, 243, 76, 319]], [[85, 165, 98, 188], [66, 167, 73, 188], [190, 210, 214, 226], [8, 168, 14, 176]], [[78, 48, 204, 249], [55, 0, 120, 81], [197, 0, 301, 53], [96, 0, 155, 66]]]\n"
          ]
        }
      ],
      "source": [
        "print(labels[:5])\n",
        "print(labels_rest[:5])\n",
        "print(annotations[:5])\n",
        "# print(annotations_door[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmmeHU9uXL4D"
      },
      "source": [
        "Processing images in COCO - Making sure that very small humans are not taken into consideration in the model (harm the results a lot)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-VP8ZJBdu_L"
      },
      "outputs": [],
      "source": [
        "images_coco = []\n",
        "labels_coco = []\n",
        "annotations_coco = []\n",
        "val = 0\n",
        "for img in images_rest:\n",
        "    cnt = 0\n",
        "    label_here = []\n",
        "    annotation_here = []\n",
        "    z = 0\n",
        "    if len(annotations[val]) > 4:\n",
        "      val = val + 1\n",
        "      continue\n",
        "    for label in annotations[val]:\n",
        "      # try:\n",
        "        # Checking width and person class\n",
        "        height, width = img.shape[:2]\n",
        "        if ((label[2]-label[0])*(label[3]-label[1]))/(width*height) < 0.005:\n",
        "            continue\n",
        "      # except:\n",
        "        # print(label)\n",
        "        annotation_here.append(label)\n",
        "        label_here.append(labels_rest[val][z])\n",
        "        cnt += 1\n",
        "        z += 1\n",
        "    val += 1\n",
        "    if cnt > 0:\n",
        "        images_coco.append(img)\n",
        "        labels_coco.append(label_here)\n",
        "        annotations_coco.append(annotation_here)\n",
        "\n",
        "# print(len(images_coco))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc6esVs7XXPF"
      },
      "source": [
        "Removing weird images in the dataset (Black & White/malformed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uDdPBx3L8wZ"
      },
      "outputs": [],
      "source": [
        "def remove_wrong_images(images, labels, annotations = None):\n",
        "    images_new = []\n",
        "    labels_new = []\n",
        "    annotations_new = []\n",
        "    cnt = 0\n",
        "    for cnt in range(len(images)):\n",
        "        # Checking malformed images\n",
        "        if len(images[cnt].shape) < 3 or images[cnt].shape[2] != 3:\n",
        "            continue\n",
        "        images_new.append(images[cnt])\n",
        "        labels_new.append(labels[cnt])\n",
        "        if annotations is not None:\n",
        "            annotations_new.append(annotations[cnt])\n",
        "    return images_new, labels_new, annotations_new\n",
        "\n",
        "images_coco, annotations_coco, labels_coco = remove_wrong_images(images_coco, annotations_coco, labels_coco)\n",
        "# images_door, annotations_door, _ = remove_wrong_images(images_door, annotations_door)\n",
        "images, labels, _ = remove_wrong_images(images, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6maUb7MxpgQz"
      },
      "outputs": [],
      "source": [
        "def image_augmentation_flip_horizontal(images):\n",
        "  original_images = images\n",
        "  new_images = []\n",
        "  for idx in range(len(new_images)):\n",
        "    image = cv2.flip(new_images[idx], 1)#*255\n",
        "    new_images.append(image)\n",
        "    # cv2_imshow(image)\n",
        "  original_images.extend(new_images)\n",
        "  return original_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsQ6cG6PszoV"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "def image_augmentation_translation(images_list, labels_list, annotations_list):\n",
        "  original_images_list = copy.deepcopy(images_list)\n",
        "  original_labels_list = copy.deepcopy(labels_list)\n",
        "  original_annotations_list = copy.deepcopy(annotations_list)\n",
        "  new_images_list = []\n",
        "  new_labels_list = []\n",
        "  new_annotations_list = []\n",
        "  for img, labels, annotations in zip(images_list, labels_list, annotations_list):\n",
        "    # annotations = [annotations]\n",
        "    height, width = img.shape[:2]\n",
        "    translation_factor = 0.2\n",
        "    max_horizontal = translation_factor*width\n",
        "    max_vertical = translation_factor*height\n",
        "    horizontal_shift = int(random.uniform(-max_horizontal, max_horizontal))\n",
        "    vertical_shift = int(random.uniform(-max_vertical, max_vertical))\n",
        "    # M[0][2] = how much right\n",
        "    # M[1][2] = how much down\n",
        "    M = np.float32([[1, 0, horizontal_shift], [0, 1, vertical_shift]])\n",
        "    new_img = cv2.warpAffine(img, M, (width, height))\n",
        "    # filling blank spaces in image\n",
        "    if horizontal_shift >= 0:\n",
        "      new_img[:,:horizontal_shift] = np.random.rand(320, horizontal_shift, 3)\n",
        "    else:\n",
        "     new_img[:,horizontal_shift:] = np.random.rand(320, -horizontal_shift, 3)\n",
        "    if vertical_shift >= 0:\n",
        "      new_img[:vertical_shift,:] = np.random.rand(vertical_shift, 320,3)\n",
        "    else:\n",
        "      new_img[vertical_shift:,:] = np.random.rand(-vertical_shift, 320,3)\n",
        "\n",
        "    # new_images.append(new_img)\n",
        "    temp_label = []\n",
        "    temp_annotations = []\n",
        "    for label, box in zip(labels, annotations):\n",
        "      x1 = box[0]\n",
        "      y1 = box[1]\n",
        "      x2 = box[2]\n",
        "      y2 = box[3]\n",
        "      x1 = x1 + horizontal_shift\n",
        "      y1 = y1 + vertical_shift\n",
        "      x2 = x2 + horizontal_shift\n",
        "      y2 = y2 + vertical_shift\n",
        "      if x1 >= 0 and x1 < width and y1 >= 0 and y1 < height and x2 >= 0 and x2 < width and y2 >= 0 and y2 < height:\n",
        "        temp_label.append(label) # No cutting\n",
        "        temp_annotations.append([x1, y1, x2, y2]) # No cutting just translation\n",
        "      elif x1 >= width or y1 >= height or x2 < 0 or y2 < 0:\n",
        "        pass # whole out\n",
        "      else: # for cases in which there is partial image cut [if >=30% is image has gone out of bounds then remove else make modifications and add]\n",
        "        # find new coordinates of bbox\n",
        "        initial_area = (x2-x1)*(y2-y1)\n",
        "        if x1 < 0:\n",
        "          x1 = 0\n",
        "        if y1 < 0:\n",
        "          y1 = 0\n",
        "        if x2 >= width:\n",
        "          x2 = width - 1\n",
        "        if y2 >= height:\n",
        "          y2 = height - 1\n",
        "        final_area = (x2-x1)*(y2-y1)\n",
        "        # check if more than 50 % has gone out\n",
        "        if final_area >= 0.7*initial_area:\n",
        "          # accept modified\n",
        "          temp_annotations.append([x1, y1, x2, y2])\n",
        "          temp_label.append(label)\n",
        "        else:\n",
        "          pass\n",
        "        # do something \n",
        "    if len(temp_annotations) > 0:\n",
        "      new_images_list.append(new_img)\n",
        "      new_labels_list.append(temp_label)\n",
        "      new_annotations_list.append(temp_annotations)\n",
        "\n",
        "    # print(horizontal_shift, vertical_shift)\n",
        "    # cv2_imshow(translated)\n",
        "  result_images_list = []\n",
        "  result_labels_list = []\n",
        "  result_annotations_list = []\n",
        "  for img1, img2 in zip(original_images_list, new_images_list):\n",
        "    result_images_list.append(img1)\n",
        "    result_images_list.append(img2)\n",
        "  for label1, label2 in zip(original_labels_list, new_labels_list):\n",
        "    result_labels_list.append(label1)\n",
        "    result_labels_list.append(label2)\n",
        "  for annotation1, annotation2 in zip(original_annotations_list, new_annotations_list):\n",
        "    result_annotations_list.append(annotation1)\n",
        "    result_annotations_list.append(annotation2)\n",
        "\n",
        "  return result_images_list, result_labels_list, result_annotations_list\n",
        "\n",
        "index1 = random.randint(0, len(images_coco))\n",
        "index2 = random.randint(0, len(images_coco))\n",
        "index3 = random.randint(0, len(images_coco))\n",
        "dummy_img_list = [images_coco[index1], images_coco[index2], images_coco[index3]]\n",
        "dummy_class_list = [labels_coco[index1], labels_coco[index2], labels_coco[index3]]\n",
        "dummy_annotations_list = [annotations_coco[index1], annotations_coco[index2], annotations_coco[index3]] \n",
        "\n",
        "# dummy_img_list, dummy_class_list, dummy_annotations_list = image_augmentation_translation(dummy_img_list, dummy_class_list, dummy_annotations_list)\n",
        "# images, dummy_class_list, labels = image_augmentation_translation(images, [[1]]*len(images), labels)\n",
        "# images_coco, labels_coco, annotations_coco = image_augmentation_translation(images_coco, labels_coco, annotations_coco)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nkw5ibUZ_r-E"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "def img_list_viewer(images_list, labels_list, annotations_list):\n",
        "  for img, labels, annotations in zip(images_list, labels_list, annotations_list):\n",
        "    temp_img = copy.deepcopy(img)\n",
        "    # print(labels, annotations)\n",
        "    for label, annotation in zip(labels, annotations):\n",
        "      # print(label, annotation)\n",
        "      color = None\n",
        "      if label == \"doll\":\n",
        "        color = (1, 0, 0)\n",
        "      elif label == \"fire hydrant\":\n",
        "        color = (0, 1 ,0)\n",
        "      elif label == \"person\":\n",
        "        color = (0, 0 ,1)\n",
        "      # print(annotations)\n",
        "      temp_img = cv2.rectangle(temp_img, (annotation[0], annotation[1]), (annotation[2], annotation[3]), color, 2)\n",
        "    temp_img = temp_img*255\n",
        "    temp_img = np.array(temp_img, dtype=np.uint8)\n",
        "    temp_img = cv2.cvtColor(temp_img, cv2.COLOR_RGB2BGR)\n",
        "    cv2_imshow(temp_img)\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LVOLrhGWzIV"
      },
      "outputs": [],
      "source": [
        "dummy_img, dummy_label, dummy_annotation = image_augmentation_translation(images[:5], [[\"doll\"]]*len(labels[:5]), labels[:5])\n",
        "img_list_viewer(dummy_img, dummy_label, dummy_annotation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgI1Kg36aBGP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4b00bec-3ea6-4590-b19f-a3bdec36a3cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of doll samples :  459\n",
            "No of person samples :  481\n",
            "No of fire hydrant samples :  305\n"
          ]
        }
      ],
      "source": [
        "# counting no of samples per class\n",
        "print(\"No. of doll samples : \", len(images))\n",
        "count_person = 0\n",
        "count_fire_hydrant = 0\n",
        "flat_labels_coco = [item for sublist in labels_coco for item in sublist]\n",
        "for class_name in flat_labels_coco:\n",
        "  if class_name == \"person\":\n",
        "    count_person = count_person + 1\n",
        "  elif class_name == \"fire hydrant\":\n",
        "    count_fire_hydrant = count_fire_hydrant + 1\n",
        "# print(flat_labels_coco[:5])\n",
        "print(\"No of person samples : \", count_person)\n",
        "print(\"No of fire hydrant samples : \", count_fire_hydrant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAG_jpgBWv7q"
      },
      "outputs": [],
      "source": [
        "dummy_img, dummy_label, dummy_annotation = image_augmentation_translation(images_rest[:5], labels_rest[:5], annotations[:5])\n",
        "img_list_viewer(dummy_img, dummy_label, dummy_annotation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3xk-ivjYX8u"
      },
      "outputs": [],
      "source": [
        "images, dummy_list, labels = image_augmentation_translation(images, [[\"doll\"]]*len(labels), labels)\n",
        "images_coco, labels_coco, annotations_coco= image_augmentation_translation(images_coco, labels_coco, annotations_coco)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6OF0kUgvgcOs"
      },
      "outputs": [],
      "source": [
        "images, dummy_list, labels = image_augmentation_translation(images, [[\"doll\"]]*len(labels), labels)\n",
        "images_coco, labels_coco, annotations_coco= image_augmentation_translation(images_coco, labels_coco, annotations_coco)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvV5jjdxrduM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b67b152-2d97-438e-db56-2f3516981301"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of doll samples :  1608\n",
            "No of person samples :  1655\n",
            "No of fire hydrant samples :  1073\n"
          ]
        }
      ],
      "source": [
        "# counting no of samples per class\n",
        "print(\"No. of doll samples : \", len(images))\n",
        "count_person = 0\n",
        "count_fire_hydrant = 0\n",
        "flat_labels_coco = [item for sublist in labels_coco for item in sublist]\n",
        "for class_name in flat_labels_coco:\n",
        "  if class_name == \"person\":\n",
        "    count_person = count_person + 1\n",
        "  elif class_name == \"fire hydrant\":\n",
        "    count_fire_hydrant = count_fire_hydrant + 1\n",
        "# print(flat_labels_coco[:5])\n",
        "print(\"No of person samples : \", count_person)\n",
        "print(\"No of fire hydrant samples : \", count_fire_hydrant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPcfv1ggXpyB"
      },
      "source": [
        "## Data Augmentation\n",
        "The following image transform classes were defined to deal with bounding boxes in transformations.\n",
        "1. Random Crop (resized to full)\n",
        "2. Perspective Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIniPUxx-i0G"
      },
      "outputs": [],
      "source": [
        "# For reference, see PyTorch's implementation of T.RandomResizedCrop\n",
        "class RandomResizedCropWithBox(T.RandomResizedCrop):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(RandomResizedCropWithBox, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, img_data):\n",
        "        img = img_data[0]\n",
        "        boxes = img_data[1]\n",
        "        classes = img_data[2]\n",
        "        i, j, h, w = self.get_params(img, self.scale, self.ratio)\n",
        "        num_boxes = len(boxes)\n",
        "        new_labels = []\n",
        "        new_classes = []\n",
        "\n",
        "        # Creating labels for each bounding box\n",
        "        for val in range(num_boxes):\n",
        "            xmin, ymin, xmax, ymax = boxes[val]\n",
        "            # Checking if it lies inside the cropped image\n",
        "            if xmax <= j or xmin >= j+w or ymax <= i or ymin >= i+h:\n",
        "                continue\n",
        "            x1 = (max(xmin, j)-j)*320/w\n",
        "            x2 = (min(xmax, j+w)-j)*320/w\n",
        "            y1 = (max(ymin, i)-i)*320/h\n",
        "            y2 = (min(ymax, i+h)-i)*320/h\n",
        "            for value in (x1, x2, y1, y2):\n",
        "                if value < 0:\n",
        "                    value = 0\n",
        "                if value >= 320:\n",
        "                    value = 319\n",
        "            new_labels.append([x1, y1, x2, y2])\n",
        "            new_classes.append(classes[val])\n",
        "        new_labels = torch.from_numpy(np.array(new_labels)).int()\n",
        "\n",
        "        # Returns resized image, labels (box co-ordinates) and classes\n",
        "        return [torchvision.transforms.functional.resized_crop(img, i, j, h, w, self.size, self.interpolation), new_labels, new_classes]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqE5_i27RnKv"
      },
      "source": [
        "The custom dataset class which outputs objects according to `idx` - Different ranges give objects from different arrays above.\n",
        "\n",
        "`collate_fn` is used to get the correct format of values from the dataloader (to handle the dictionaries correctly)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TPEkaOA5Axvf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31c4ab8e-eff2-4484-afb2-b272a10b41a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1286 1156\n",
            "322 290\n"
          ]
        }
      ],
      "source": [
        "class DollDataset(Dataset):\n",
        "\n",
        "    # All arrays and values that are part of the Dataset class\n",
        "    def __init__(self, images, labels, images_coco, labels_coco, annotations, xsize, ysize, perspective_prob = 0.5):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.images_rest = images_coco\n",
        "        self.labels_rest = labels_coco\n",
        "        self.annotations = annotations\n",
        "        # self.images_doors = images_doors\n",
        "        # self.labels_doors = labels_doors\n",
        "        self.xsize = xsize\n",
        "        self.ysize = ysize\n",
        "        self.perspective_prob = perspective_prob\n",
        "    \n",
        "    # Combining 3 arrays\n",
        "    def __len__(self):\n",
        "        return len(self.images)+len(self.images_rest)#+len(self.images_doors)\n",
        "\n",
        "    # Crucial function, returns (non-)transformed image with box\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        # Normalize with calculated mean and std dev\n",
        "        trans = T.Compose([T.ToTensor(), T.Resize((self.xsize,self.ysize)), T.Normalize([0.4662, 0.4279, 0.3946], [0.2736, 0.2650, 0.2774])])\n",
        "        # trans = T.Compose([T.ToTensor(), T.Resize((self.xsize,self.ysize))])\n",
        "\n",
        "        # Dealing with individual arrays\n",
        "        z = len(self.images)\n",
        "        z2 = len(self.images_rest)\n",
        "        # print(z, z+z2)\n",
        "        if idx < z:\n",
        "            class_list = [1]\n",
        "        elif idx < z+z2:\n",
        "            class_list = self.labels_rest[idx-z]\n",
        "            for i in range(len(class_list)):\n",
        "                if class_list[i] == 'person':\n",
        "                    class_list[i] = 3\n",
        "                if class_list[i] == 'fire hydrant':\n",
        "                    class_list[i] = 2\n",
        "        # else:\n",
        "        #     class_list = [3]*len(self.labels_doors[idx-z-z2])\n",
        "        try:\n",
        "          if idx < z:\n",
        "              img = self.images[idx]\n",
        "          elif idx < z+z2:\n",
        "              img = self.images_rest[idx-z]\n",
        "        except:\n",
        "          print(\"from img \", idx)\n",
        "        # else:\n",
        "        #     img = self.images_doors[idx-z-z2] \n",
        "        try:\n",
        "          if idx < z:\n",
        "              label = np.array(self.labels[idx])\n",
        "          elif idx < z+z2:\n",
        "              label = np.array(self.annotations[idx-z])\n",
        "          else:\n",
        "            print(\"from label \", idx)\n",
        "        except:\n",
        "          print(\"from label \", idx)\n",
        "        # else:\n",
        "        #     label = np.array(self.labels_doors[idx-z-z2])\n",
        "\n",
        "        # Label resizing\n",
        "        label = torch.from_numpy(label)\n",
        "        y_size, x_size = img.shape[:2]\n",
        "        label[:,1] = label[:,1]*320/y_size\n",
        "        label[:,3] = label[:,3]*320/y_size\n",
        "        label[:,0] = label[:,0]*320/x_size\n",
        "        label[:,2] = label[:,2]*320/x_size\n",
        "        label = label.int()\n",
        "        # print(img)\n",
        "        img = trans(img)\n",
        "        return (img, label, class_list)\n",
        "\n",
        "# Necessary to form a dataloader\n",
        "def collate_fn(data):\n",
        "    dics = []\n",
        "    for x in range(len(data)):\n",
        "        dic = {'image': data[x][0], 'bbox': data[x][1], 'label': torch.tensor(data[x][2])}\n",
        "        dics.append(dic)\n",
        "    return dics\n",
        "\n",
        "print(len(images)*4//5, len(images_coco)*4//5)\n",
        "\n",
        "a = len(images)*4//5\n",
        "b = len(images_coco)*4//5\n",
        "\n",
        "\n",
        "# train dataset\n",
        "random.Random(6).shuffle(images[:a])\n",
        "random.Random(6).shuffle(labels[:a])\n",
        "random.Random(6).shuffle(images_coco[:b])\n",
        "random.Random(6).shuffle(labels_coco[:b])\n",
        "random.Random(6).shuffle(annotations_coco[:b])\n",
        "train_set = DollDataset(images[:a], labels[:a], images_coco[:b], labels_coco[:b], annotations_coco[:b], 320, 320, perspective_prob=-1)\n",
        "\n",
        "print(len(images[a:]), len(images_coco[b:]))\n",
        "# test dataset\n",
        "random.Random(2).shuffle(images[a:])\n",
        "random.Random(2).shuffle(labels[a:])\n",
        "random.Random(2).shuffle(images_coco[b:])\n",
        "random.Random(2).shuffle(labels_coco[b:])\n",
        "random.Random(2).shuffle(annotations_coco[b:])\n",
        "val_set = DollDataset(images[a:], labels[a:], images_coco[b:], labels_coco[b:], annotations_coco[b:], 320, 320, perspective_prob=-1)\n",
        "\n",
        "# print(train_set.__len__())\n",
        "# print(val_set.__len__())\n",
        "\n",
        "# Training and Validation\n",
        "# train_set, _ = torch.utils.data.random_split(doll_set1, [doll_set1.__len__()*1, 0])\n",
        "# val_set, _ = torch.utils.data.random_split(doll_set2, [doll_set2.__len__()*1, 0])\n",
        "train_loader = DataLoader(train_set, batch_size = 16, shuffle = True, collate_fn = collate_fn, drop_last = True)\n",
        "val_loader = DataLoader(val_set, batch_size = 16, shuffle = True, collate_fn = collate_fn, drop_last = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loB69wuIN2sE"
      },
      "outputs": [],
      "source": [
        "# print(doll_set[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "690iQvHTN-hk"
      },
      "outputs": [],
      "source": [
        "# print(doll_set[7][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NhUSqFMaW3Iq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3761ccda-5552-4bfe-80f0-d882b8b1a161"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2442 612\n"
          ]
        }
      ],
      "source": [
        "print(len(train_set), len(val_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gclqEG9sll6g"
      },
      "outputs": [],
      "source": [
        "# train_set[2430][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7evOs8zbZOTF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "5a91fd22f25c49bbb09ba83d122ac8e3",
            "5e22dd7945ab406d92518d1044abe7d6",
            "22d58e3fbf2f441f8a4ff125d7d72b84",
            "25f73edd9b984fa7930cf79b87a46cb7",
            "f3645b31aa08478da67d73c28bccc401",
            "d70e8e2c7e424ad69a93b8b6193dfb47",
            "c4eff422c0534177818c6b9934c7ca4f",
            "0859e2e5d9b0475ea9f6ef654383ec65",
            "bc9867aa3bb54260ae07b4a1f9fa63dc",
            "a550e3d21bf4478f83181ec22d31b42c",
            "ba796b5db5cb4ed4a89f5297ae37cc46"
          ]
        },
        "outputId": "a191a979-feb3-44ff-bd1a-f86d5bf73f29"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/152 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a91fd22f25c49bbb09ba83d122ac8e3"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Testing if the dataloader works\n",
        "for dic in tqdm(train_loader):\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I1RPLfAR_Lj"
      },
      "source": [
        "Testing the working of the dataloader using the ```show``` function for torch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zdZh8SlbLy0"
      },
      "outputs": [],
      "source": [
        "# a = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwJy5w8eghN2"
      },
      "outputs": [],
      "source": [
        "# import torchvision.transforms.functional as F\n",
        "# import random\n",
        "\n",
        "# random.seed(0)\n",
        "# torch.manual_seed(0)\n",
        "# np.random.seed(0)\n",
        "\n",
        "# # Custom function to display images\n",
        "# def show(imgs):\n",
        "#     if not isinstance(imgs, list):\n",
        "#         imgs = [imgs]\n",
        "#     fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
        "#     for i, img in enumerate(imgs):\n",
        "#         img = img.detach()\n",
        "#         img = F.to_pil_image(img)\n",
        "#         axs[0, i].imshow(np.asarray(img))\n",
        "#         axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "# from torchvision.transforms.functional import convert_image_dtype\n",
        "# from torchvision.utils import draw_bounding_boxes\n",
        "# plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
        "# # T.Normalize([70.0594, 62.4050, 58.8377], [78.0825, 72.5514, 73.4684]\n",
        "# # Showing one batch of images coming from the data loader\n",
        "# for dic in tqdm(val_loader):\n",
        "#     for x in range(len(dic)):\n",
        "#         z = dic[x]['image']\n",
        "#         z[0] = (z[0]*0.2736+0.4662)*255\n",
        "#         z[1] = (z[1]*0.2650+0.4279)*255\n",
        "#         z[2] = (z[2]*0.2774+0.3946)*255\n",
        "#         # print(type(z))\n",
        "#         # print(z)\n",
        "#         bbox = dic[x]['bbox']\n",
        "#         # print(bbox)\n",
        "#         boxes = []\n",
        "#         boxes.append(torch.tensor([0,0,0,0]))\n",
        "#         for al in range(len(bbox)):\n",
        "#             boxes.append(bbox[al])\n",
        "#         img=draw_bounding_boxes(z.type(torch.uint8), boxes=torch.vstack(boxes), width=4)\n",
        "#         # img = \n",
        "#         show(img)\n",
        "#         break\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjrcFUFozsXj"
      },
      "source": [
        "### Useful functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iou_threshold = 0.5 # for mAP and Confusion matrix\n",
        "nms_iou_threshold = 0.3"
      ],
      "metadata": {
        "id": "mpV20elvbk1a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAnAM__Kzw-_"
      },
      "outputs": [],
      "source": [
        "# Calculates IOU\n",
        "def iou(box1, box2):\n",
        "    # box1 = list(map(lambda x: int(x), box1))\n",
        "    # box2 = list(map(lambda x: int(x), box2))\n",
        "    a1 = (box1[2]-box1[0])*(box1[3]-box1[1])\n",
        "    a2 = (box2[2]-box2[0])*(box2[3]-box2[1])\n",
        "    inter = max(0, min(box1[2], box2[2]) - max(box1[0], box2[0])) * max(0, min(box1[3], box2[3]) - max(box1[1], box2[1]))\n",
        "    return inter/(a1 + a2 - inter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "577tX6PKyeE5"
      },
      "outputs": [],
      "source": [
        "def mean_average_precision(output, target, iou_threshold = iou_threshold, starting_class = 1, ending_class = 4):\n",
        "    final_map = 0\n",
        "    final_map_1 = 0\n",
        "    final_map_2 = 0\n",
        "    final_map_3 = 0\n",
        "    # ap_per_class = [0, 0, 0, 0]\n",
        "    # count = [0, 0, 0, 0]\n",
        "    for k in range(len(output)):\n",
        "      confidence = [[],[],[],[]]\n",
        "      pos_neg = [[],[],[],[]]\n",
        "      if output[k]['labels'].size()[0] == 0:\n",
        "          if target[k]['labels'].size()[0] == 0:\n",
        "              final_map += 1\n",
        "              continue\n",
        "          else:\n",
        "              final_map += 0\n",
        "              continue\n",
        "      for i in range(output[k]['labels'].size()[0]):\n",
        "          confidence[output[k]['labels'][i]].append(float(output[k]['scores'][i]))\n",
        "          pos_neg[output[k]['labels'][i]].append(False)\n",
        "          for j in range(target[k]['labels'].size()[0]):\n",
        "              if target[k]['labels'][j] != output[k]['labels'][i]:\n",
        "                  continue\n",
        "              if iou(target[k]['boxes'][j].tolist(), output[k]['boxes'].int()[i].tolist()) > iou_threshold:\n",
        "                  pos_neg[output[k]['labels'][i]][-1] = True\n",
        "                  break\n",
        "      # thresholds = np.arange(start=0.2, stop=0.7, step=0.05)\n",
        "\n",
        "      sum_ap = 0\n",
        "      cnt_ap = 0\n",
        "      for cls in range(starting_class, ending_class+1):\n",
        "          if len(confidence[cls]) == 0:\n",
        "              continue\n",
        "          cnt_ap += 1\n",
        "          if True not in pos_neg[cls]:\n",
        "              ap = 0\n",
        "          else:\n",
        "              ap = average_precision_score(pos_neg[cls], confidence[cls])\n",
        "          if math.isnan(ap):\n",
        "              print(pos_neg[cls], confidence[cls])\n",
        "          sum_ap += ap\n",
        "          # print(\"AP for class \", cls, \" is \", ap)\n",
        "          # ap_per_class[cls] = ap_per_class[cls] + ap\n",
        "          # count[cls] += count[cls] + 1\n",
        "\n",
        "      if cnt_ap == 0:\n",
        "          final_map += 1\n",
        "          # ap_per_class[cls] = ap_per_class[cls] + 1\n",
        "          # count[cls] += count[cls] + 1\n",
        "      else:\n",
        "          final_map +=  sum_ap/cnt_ap\n",
        "    # print([\"does not matter\", ap_per_class[1]/count[1], ap_per_class[2]/count[2], ap_per_class[3]/count[3]])\n",
        "    return final_map/len(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K7bJE8JjGjW9"
      },
      "outputs": [],
      "source": [
        "# def filter_confidence(output, threshold = 0.5):\n",
        "#   filtered = []\n",
        "#   for idx in range(len(output)):\n",
        "#     boxes = output[idx][\"boxes\"]\n",
        "#     scores = output[idx][\"scores\"]\n",
        "#     labels = output[idx][\"labels\"]\n",
        "#     new_boxes = []\n",
        "#     new_scores = []\n",
        "#     new_labels = []\n",
        "#     for box, score, label in zip(boxes, scores, labels):\n",
        "#       if score > threshold:\n",
        "#         new_boxes.append(box)\n",
        "#         new_scores.append(score)\n",
        "#         new_labels.append(label)\n",
        "#     if len(new_scores) != 0:\n",
        "#       new_boxes = torch.stack(new_boxes)\n",
        "#       new_scores = torch.tensor(new_scores)\n",
        "#       new_labels = torch.tensor(new_labels)\n",
        "#     else:\n",
        "#       new_boxes = torch.tensor([[]])\n",
        "#       new_scores = torch.tensor([])\n",
        "#       new_labels = torch.tensor([])\n",
        "#     temp_filtered = {\"boxes\": new_boxes, \"scores\": new_scores, \"labels\": new_labels}\n",
        "#     filtered.append(temp_filtered)\n",
        "#   return filtered"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# soft nms\n",
        "def soft_nms_pytorch(dets, box_scores, sigma=0.5, thresh = 0.5, cuda=0):\n",
        "    \"\"\"\n",
        "    Build a pytorch implement of Soft NMS algorithm.\n",
        "    # Augments\n",
        "        dets:        boxes coordinate tensor (format:[x1, y1, x2, y2])\n",
        "        box_scores:  box score tensors\n",
        "        sigma:       variance of Gaussian function\n",
        "        thresh:      score thresh\n",
        "        cuda:        CUDA flag\n",
        "    # Return\n",
        "        the index of the selected boxes\n",
        "    \"\"\"\n",
        "\n",
        "    # Indexes concatenate boxes with the last column\n",
        "    N = dets.shape[0]\n",
        "    if cuda:\n",
        "        indexes = torch.arange(0, N, dtype=torch.float).cuda().view(N, 1)\n",
        "    else:\n",
        "        indexes = torch.arange(0, N, dtype=torch.float).view(N, 1)\n",
        "    dets = torch.cat((dets, indexes), dim=1)\n",
        "\n",
        "    # The order of boxes coordinate is [y1,x1,y2,x2]\n",
        "    x1 = dets[:, 0]\n",
        "    y1 = dets[:, 1]\n",
        "    x2 = dets[:, 2]\n",
        "    y2 = dets[:, 3]\n",
        "    scores = box_scores\n",
        "    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
        "\n",
        "    for i in range(N):\n",
        "        # intermediate parameters for later parameters exchange\n",
        "        tscore = scores[i].clone()\n",
        "        pos = i + 1\n",
        "\n",
        "        if i != N - 1:\n",
        "            maxscore, maxpos = torch.max(scores[pos:], dim=0)\n",
        "            if tscore < maxscore:\n",
        "                dets[i], dets[maxpos.item() + i + 1] = dets[maxpos.item() + i + 1].clone(), dets[i].clone()\n",
        "                scores[i], scores[maxpos.item() + i + 1] = scores[maxpos.item() + i + 1].clone(), scores[i].clone()\n",
        "                areas[i], areas[maxpos + i + 1] = areas[maxpos + i + 1].clone(), areas[i].clone()\n",
        "\n",
        "        # IoU calculate\n",
        "        yy1 = np.maximum(dets[i, 0].to(\"cpu\").detach().numpy(), dets[pos:, 0].to(\"cpu\").detach().numpy())\n",
        "        xx1 = np.maximum(dets[i, 1].to(\"cpu\").detach().numpy(), dets[pos:, 1].to(\"cpu\").detach().numpy())\n",
        "        yy2 = np.minimum(dets[i, 2].to(\"cpu\").detach().numpy(), dets[pos:, 2].to(\"cpu\").detach().numpy())\n",
        "        xx2 = np.minimum(dets[i, 3].to(\"cpu\").detach().numpy(), dets[pos:, 3].to(\"cpu\").detach().numpy())\n",
        "        \n",
        "        w = np.maximum(0.0, xx2 - xx1 + 1)\n",
        "        h = np.maximum(0.0, yy2 - yy1 + 1)\n",
        "        inter = torch.tensor(w * h).cuda() if cuda else torch.tensor(w * h)\n",
        "        ovr = torch.div(inter, (areas[i] + areas[pos:] - inter))\n",
        "\n",
        "        # Gaussian decay\n",
        "        weight = torch.exp(-(ovr * ovr) / sigma)\n",
        "        scores[pos:] = weight * scores[pos:]\n",
        "\n",
        "    # select the boxes and keep the corresponding indexes\n",
        "    keep = dets[:, 4][scores > thresh].int()\n",
        "\n",
        "    return keep\n"
      ],
      "metadata": {
        "id": "qNf-gqsbV00Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nAVYgX9R8kB"
      },
      "outputs": [],
      "source": [
        "def batch_nms_confidence_filter(ls, hard = True, conf_threhold = 0.5):\n",
        "  for idx in range(len(ls)):\n",
        "    \n",
        "    if hard: # does not give confidence filtered output\n",
        "      ls_index = torchvision.ops.batched_nms(boxes = ls[idx][\"boxes\"], scores = ls[idx][\"scores\"], idxs = ls[idx][\"labels\"], iou_threshold = nms_iou_threshold)\n",
        "      confidence_filtered_ls_index = []\n",
        "      for index in ls_index:\n",
        "        if ls[idx][\"scores\"][index] > conf_threhold:\n",
        "          confidence_filtered_ls_index.append(index)\n",
        "      ls_index = confidence_filtered_ls_index\n",
        "    else:    # gives confidence filtered output\n",
        "      ls_index = soft_nms_pytorch(ls[idx][\"boxes\"], ls[idx][\"scores\"], thresh = conf_threhold) \n",
        "    \n",
        "    temp_ls = {\"boxes\":[], \"scores\":[], \"labels\":[]}\n",
        "    \n",
        "    for index in ls_index:\n",
        "      temp_ls[\"boxes\"].append(ls[idx][\"boxes\"][index])\n",
        "      temp_ls[\"scores\"].append(ls[idx][\"scores\"][index])\n",
        "      temp_ls[\"labels\"].append(ls[idx][\"labels\"][index])\n",
        "    if len(temp_ls[\"scores\"]) > 0:\n",
        "      temp_ls[\"boxes\"] = torch.stack(temp_ls[\"boxes\"])\n",
        "      temp_ls[\"scores\"] = torch.tensor(temp_ls[\"scores\"])\n",
        "      temp_ls[\"labels\"] = torch.tensor(temp_ls[\"labels\"])\n",
        "    else:\n",
        "      temp_ls[\"boxes\"] = torch.tensor([[]])\n",
        "      temp_ls[\"scores\"] = torch.tensor([])\n",
        "      temp_ls[\"labels\"] = torch.tensor([])\n",
        "    ls[idx] = temp_ls\n",
        "  return ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def confusion_matrix(ls, targets_model, TP, FP, FN, TN, iou_threshold = iou_threshold):\n",
        "      for x in range(len(ls)):\n",
        "        predicted_scores = ls[x][\"scores\"].detach().to(\"cpu\").numpy() \n",
        "        predicted_labels = ls[x][\"labels\"].detach().to(\"cpu\").numpy() \n",
        "        predicted_boxes = ls[x][\"boxes\"].detach().to(\"cpu\").numpy()\n",
        "        # ground_scores = targets_model[x][\"scores\"].detach().to(\"cpu\").numpy() \n",
        "        ground_labels = targets_model[x][\"labels\"].detach().to(\"cpu\").numpy() \n",
        "        ground_boxes =  targets_model[x][\"boxes\"].detach().to(\"cpu\").numpy() \n",
        "        for index, (predicted_box, predicted_label) in enumerate(zip(predicted_boxes, predicted_labels)):\n",
        "          for idx, (ground_box, ground_label) in enumerate(zip(ground_boxes, ground_labels)):\n",
        "            if predicted_label == ground_label and iou(predicted_box, ground_box) > iou_threshold:\n",
        "              TP += 1\n",
        "              ground_labels[idx] = -1\n",
        "              predicted_labels[index] = -2\n",
        "              break\n",
        "\n",
        "        not_counted = 0\n",
        "        for label in ground_labels:\n",
        "          if label != -1:\n",
        "            not_counted += 1\n",
        "        FN += not_counted\n",
        "\n",
        "        false_counted = 0\n",
        "        for label in predicted_labels:\n",
        "          if label != -2:\n",
        "            false_counted += 1\n",
        "        FP += false_counted\n",
        "      return (TP, FP, FN, TN)"
      ],
      "metadata": {
        "id": "WXgVgy1HgnCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEOS19dgznx6"
      },
      "source": [
        "## model loading"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# my_model = torch.load(\"/content/drive/MyDrive/SSD detection/SSD detection_3classes_4Xdata_augmented.pth\")\n",
        "# model = my_model\n",
        "# model.eval()"
      ],
      "metadata": {
        "id": "0bDeUTry2LcP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5BKd0964fxx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ff11c7c-e133-4435-9137-d1b896afbc2c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SSD(\n",
              "  (backbone): SSDLiteFeatureExtractorMobileNet(\n",
              "    (features): Sequential(\n",
              "      (0): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
              "              (1): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
              "              (1): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (2): ConvNormActivation(\n",
              "              (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (3): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(72, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
              "              (1): BatchNorm2d(72, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (2): ConvNormActivation(\n",
              "              (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (4): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(72, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
              "              (1): BatchNorm2d(72, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): ReLU()\n",
              "              (scale_activation): Hardsigmoid()\n",
              "            )\n",
              "            (3): ConvNormActivation(\n",
              "              (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(40, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (5): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(120, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
              "              (1): BatchNorm2d(120, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): ReLU()\n",
              "              (scale_activation): Hardsigmoid()\n",
              "            )\n",
              "            (3): ConvNormActivation(\n",
              "              (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(40, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (6): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(120, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
              "              (1): BatchNorm2d(120, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): ReLU()\n",
              "              (scale_activation): Hardsigmoid()\n",
              "            )\n",
              "            (3): ConvNormActivation(\n",
              "              (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(40, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (7): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(240, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
              "              (1): BatchNorm2d(240, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (2): ConvNormActivation(\n",
              "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (8): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(200, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
              "              (1): BatchNorm2d(200, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (2): ConvNormActivation(\n",
              "              (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (9): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(184, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
              "              (1): BatchNorm2d(184, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (2): ConvNormActivation(\n",
              "              (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (10): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(184, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
              "              (1): BatchNorm2d(184, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (2): ConvNormActivation(\n",
              "              (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (11): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): ReLU()\n",
              "              (scale_activation): Hardsigmoid()\n",
              "            )\n",
              "            (3): ConvNormActivation(\n",
              "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(112, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (12): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
              "              (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): ReLU()\n",
              "              (scale_activation): Hardsigmoid()\n",
              "            )\n",
              "            (3): ConvNormActivation(\n",
              "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(112, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (13): ConvNormActivation(\n",
              "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): Sequential(\n",
              "          (1): ConvNormActivation(\n",
              "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
              "            (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): ConvNormActivation(\n",
              "            (0): Conv2d(672, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
              "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): ReLU()\n",
              "              (scale_activation): Hardsigmoid()\n",
              "            )\n",
              "            (3): ConvNormActivation(\n",
              "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
              "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): ReLU()\n",
              "              (scale_activation): Hardsigmoid()\n",
              "            )\n",
              "            (3): ConvNormActivation(\n",
              "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (3): ConvNormActivation(\n",
              "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (extra): ModuleList(\n",
              "      (0): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(480, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
              "          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): ConvNormActivation(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
              "          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): ConvNormActivation(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
              "          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): ConvNormActivation(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (3): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
              "          (1): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): ConvNormActivation(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (anchor_generator): DefaultBoxGenerator(aspect_ratios=[[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]], clip=True, scales=[0.2, 0.35, 0.5, 0.65, 0.8, 0.95, 1.0], steps=None)\n",
              "  (head): SSDLiteHead(\n",
              "    (classification_head): SSDLiteClassificationHead(\n",
              "      (module_list): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): ConvNormActivation(\n",
              "            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
              "            (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(672, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): ConvNormActivation(\n",
              "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "            (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(480, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): Sequential(\n",
              "          (0): ConvNormActivation(\n",
              "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
              "            (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (3): Sequential(\n",
              "          (0): ConvNormActivation(\n",
              "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "            (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(256, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (4): Sequential(\n",
              "          (0): ConvNormActivation(\n",
              "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "            (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(256, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (5): Sequential(\n",
              "          (0): ConvNormActivation(\n",
              "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
              "            (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(128, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (regression_head): SSDLiteRegressionHead(\n",
              "      (module_list): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): ConvNormActivation(\n",
              "            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
              "            (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(672, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): ConvNormActivation(\n",
              "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "            (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(480, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): Sequential(\n",
              "          (0): ConvNormActivation(\n",
              "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
              "            (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (3): Sequential(\n",
              "          (0): ConvNormActivation(\n",
              "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "            (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(256, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (4): Sequential(\n",
              "          (0): ConvNormActivation(\n",
              "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "            (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(256, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (5): Sequential(\n",
              "          (0): ConvNormActivation(\n",
              "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
              "            (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(128, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
              "      Resize(min_size=(320,), max_size=320, mode='bilinear')\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "my_model = torch.load(\"/content/drive/MyDrive/SSD detection/models/SSD detection_3classes_2Xdata_augmented.pth\")\n",
        "model = my_model\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "x1qCJb9GW_2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WOfv4PxaIWm"
      },
      "outputs": [],
      "source": [
        "# vaibhav_model = torch.load(\"/content/drive/MyDrive/SSD detection/vaibhav_model_dump.pth\")\n",
        "# model = vaibhav_model\n",
        "# model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVIZ2q7Bzi0d"
      },
      "source": [
        "## mAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTzISp5esVw8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "6ea1094ca75c4cc1b2308e7c41515441",
            "938d64f156564a3f9e947c1a2861684d",
            "9dcb33f4d4f847579ec6d6b818142799",
            "d83c06ea673443798b6df57189bff5b6",
            "828455a281e34236b916f2c863f94eed",
            "4c767521ba1747a3824c902849c2498c",
            "4455ad15ec8e47a1bb6723bc1a30e3aa",
            "cde486c36e584499a5d141395821d78c",
            "0f9e2e4465f549fb8c914e760924a6f4",
            "6118375b56324863a29973accd3455a9",
            "7b39315ff0da42088e85d203167420ad"
          ]
        },
        "outputId": "adf78b9a-8166-48a4-b98f-f707191099d8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/38 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ea1094ca75c4cc1b2308e7c41515441"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For class 1 map:0.8774671052631579 | For class 2 map : 0.8536184210526315 | For class 3 map : 0.8338815789473685\n",
            "Overall map :0.7952302631578948\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "map_class_all = 0\n",
        "map_class1 = 0\n",
        "map_class2 = 0\n",
        "map_class3 = 0\n",
        "updates = 0\n",
        "for dic in tqdm(val_loader):\n",
        "    images_model = []\n",
        "    targets_model = []\n",
        "    for x in range(len(dic)):\n",
        "        images_model.append(dic[x]['image'].float())\n",
        "        dictionary = {'boxes': dic[x]['bbox'], 'labels': dic[x]['label']}\n",
        "        targets_model.append(dictionary)\n",
        "    ls = model.forward(images_model)\n",
        "    a = ls\n",
        "    b = targets_model\n",
        "    ls = batch_nms_confidence_filter(ls, hard = True)\n",
        "    map_class1 += mean_average_precision(ls, targets_model, iou_threshold, 1,1)\n",
        "    map_class2 += mean_average_precision(ls, targets_model, iou_threshold, 2,2)\n",
        "    map_class3 += mean_average_precision(ls, targets_model, iou_threshold, 3,3)\n",
        "    map_class_all += mean_average_precision(ls, targets_model, iou_threshold, 1,3)\n",
        "    # confusion_matrix\n",
        "    # map = map + c\n",
        "    updates += 1\n",
        "print(\"For class 1 map:{} | For class 2 map : {} | For class 3 map : {}\".format(map_class1/updates, map_class2/updates, map_class3/updates))\n",
        "print(\"Overall map :{}\".format(map_class_all/updates))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOEGt-Mlzev1"
      },
      "source": [
        "## CONFUSION MATRIX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ra9mHJjpS7rb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "1e078e9eb10d466baead78a73dae7bcf",
            "71c0d84a13f0435e92f7aa3f7c6af47d",
            "c3d3c9433ab340029c1a85c568ca6a43",
            "0859cf4eaec34979b32886eee6e42f76",
            "be1434fa192444bca0042d452ae68579",
            "fd68196d0eed43a9a2925fed15955106",
            "afdbf4df85ab49ca829705dadd382557",
            "6984806ddf14459b849d71086776ab13",
            "a8ac3e5accaf48b7aae70e9f6e5299bf",
            "8bef0821a13c4d649b09ca275fe67de4",
            "2cd9e7df376c4a9e9b30a77903c54ca3"
          ]
        },
        "outputId": "728c2f80-414a-4288-c9a3-e78514b1a19b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/38 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e078e9eb10d466baead78a73dae7bcf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TP = 595 | FP = 100\n",
            "-----------------\n",
            "FN = 233 | TN = Not defined\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "map = 0\n",
        "updates = 0\n",
        "val_loader_idx = 0\n",
        "\n",
        "TP = 0\n",
        "FP = 0\n",
        "FN = 0\n",
        "TN = 0\n",
        "\n",
        "for dic in tqdm(val_loader):\n",
        "    images_model = []\n",
        "    targets_model = []\n",
        "    for x in range(len(dic)):\n",
        "        images_model.append(dic[x]['image'].float())\n",
        "        dictionary = {'boxes': dic[x]['bbox'], 'labels': dic[x]['label']}\n",
        "        targets_model.append(dictionary)\n",
        "    ls = model.forward(images_model)\n",
        "    ls = batch_nms_confidence_filter(ls, hard = True)\n",
        "    TP, FP, FN, TN = confusion_matrix(ls, targets_model, TP, FP, FN, TN, iou_threshold)\n",
        "\n",
        "print(\"TP = {} | FP = {}\".format(TP, FP))\n",
        "print(\"-----------------\")\n",
        "print(\"FN = {} | TN = {}\".format(FN, \"Not defined\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIcPWHlYGw1Q"
      },
      "outputs": [],
      "source": [
        "# model.eval()\n",
        "# map = 0\n",
        "# updates = 0\n",
        "# val_loader_idx = 0\n",
        "# for dic in tqdm(val_loader):\n",
        "#     images_model = []\n",
        "#     targets_model = []\n",
        "#     for x in range(len(dic)):\n",
        "#         images_model.append(dic[x]['image'].float())\n",
        "#         dictionary = {'boxes': dic[x]['bbox'], 'labels': dic[x]['label']}\n",
        "#         targets_model.append(dictionary)\n",
        "#     ls = model.forward(images_model)\n",
        "#     ls = batch_nms_confidence_filter(ls, hard = True)\n",
        "#     for x in range(len(ls)):\n",
        "#       z = dic[x]['image']\n",
        "#       z[0] = (z[0]*0.2736+0.4662)*255\n",
        "#       z[1] = (z[1]*0.2650+0.4279)*255\n",
        "#       z[2] = (z[2]*0.2774+0.3946)*255\n",
        "#       z = z.cpu().detach().numpy()\n",
        "#       temp_img = copy.deepcopy(z)\n",
        "#       temp_img = np.array(temp_img, dtype='uint8')\n",
        "#       temp_img1 = np.zeros((320, 320, 3), dtype=\"uint8\")\n",
        "#       for i in range(320):\n",
        "#         for j in range(320):\n",
        "#           temp_img1[j][i] = (temp_img[0][j][i], temp_img[1][j][i], temp_img[2][j][i])\n",
        "#       # print(temp_img)\n",
        "#       temp_img1 = cv2.cvtColor(temp_img1, cv2.COLOR_BGR2RGB)\n",
        "#       scores = ls[x][\"scores\"].detach().to(\"cpu\").numpy() \n",
        "#       labels = ls[x][\"labels\"].detach().to(\"cpu\").numpy() \n",
        "#       boxes = ls[x][\"boxes\"].detach().to(\"cpu\").numpy()\n",
        "#       boxes = np.array(boxes, dtype='int')\n",
        "#       for label, box in zip(labels, boxes):\n",
        "#         color = None\n",
        "#         if label == 1:\n",
        "#           color = (255, 0, 0)\n",
        "#         elif label == 2:\n",
        "#           color = (0, 255, 0)\n",
        "#         elif label == 3:\n",
        "#           color = (0, 0, 255)\n",
        "#         # print(temp_img)\n",
        "#         # print(box)\n",
        "#         temp_img1 = cv2.rectangle(temp_img1, (box[0], box[1]), (box[2], box[3]), color, 2)\n",
        "#       cv2_imshow(temp_img1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cnr6hoB4J30-"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "final model evaluation.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5a91fd22f25c49bbb09ba83d122ac8e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5e22dd7945ab406d92518d1044abe7d6",
              "IPY_MODEL_22d58e3fbf2f441f8a4ff125d7d72b84",
              "IPY_MODEL_25f73edd9b984fa7930cf79b87a46cb7"
            ],
            "layout": "IPY_MODEL_f3645b31aa08478da67d73c28bccc401"
          }
        },
        "5e22dd7945ab406d92518d1044abe7d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d70e8e2c7e424ad69a93b8b6193dfb47",
            "placeholder": "​",
            "style": "IPY_MODEL_c4eff422c0534177818c6b9934c7ca4f",
            "value": "  0%"
          }
        },
        "22d58e3fbf2f441f8a4ff125d7d72b84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0859e2e5d9b0475ea9f6ef654383ec65",
            "max": 152,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bc9867aa3bb54260ae07b4a1f9fa63dc",
            "value": 0
          }
        },
        "25f73edd9b984fa7930cf79b87a46cb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a550e3d21bf4478f83181ec22d31b42c",
            "placeholder": "​",
            "style": "IPY_MODEL_ba796b5db5cb4ed4a89f5297ae37cc46",
            "value": " 0/152 [00:00&lt;?, ?it/s]"
          }
        },
        "f3645b31aa08478da67d73c28bccc401": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d70e8e2c7e424ad69a93b8b6193dfb47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4eff422c0534177818c6b9934c7ca4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0859e2e5d9b0475ea9f6ef654383ec65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc9867aa3bb54260ae07b4a1f9fa63dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a550e3d21bf4478f83181ec22d31b42c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba796b5db5cb4ed4a89f5297ae37cc46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ea1094ca75c4cc1b2308e7c41515441": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_938d64f156564a3f9e947c1a2861684d",
              "IPY_MODEL_9dcb33f4d4f847579ec6d6b818142799",
              "IPY_MODEL_d83c06ea673443798b6df57189bff5b6"
            ],
            "layout": "IPY_MODEL_828455a281e34236b916f2c863f94eed"
          }
        },
        "938d64f156564a3f9e947c1a2861684d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c767521ba1747a3824c902849c2498c",
            "placeholder": "​",
            "style": "IPY_MODEL_4455ad15ec8e47a1bb6723bc1a30e3aa",
            "value": "100%"
          }
        },
        "9dcb33f4d4f847579ec6d6b818142799": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cde486c36e584499a5d141395821d78c",
            "max": 38,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f9e2e4465f549fb8c914e760924a6f4",
            "value": 38
          }
        },
        "d83c06ea673443798b6df57189bff5b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6118375b56324863a29973accd3455a9",
            "placeholder": "​",
            "style": "IPY_MODEL_7b39315ff0da42088e85d203167420ad",
            "value": " 38/38 [00:53&lt;00:00,  1.29s/it]"
          }
        },
        "828455a281e34236b916f2c863f94eed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c767521ba1747a3824c902849c2498c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4455ad15ec8e47a1bb6723bc1a30e3aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cde486c36e584499a5d141395821d78c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f9e2e4465f549fb8c914e760924a6f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6118375b56324863a29973accd3455a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b39315ff0da42088e85d203167420ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e078e9eb10d466baead78a73dae7bcf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_71c0d84a13f0435e92f7aa3f7c6af47d",
              "IPY_MODEL_c3d3c9433ab340029c1a85c568ca6a43",
              "IPY_MODEL_0859cf4eaec34979b32886eee6e42f76"
            ],
            "layout": "IPY_MODEL_be1434fa192444bca0042d452ae68579"
          }
        },
        "71c0d84a13f0435e92f7aa3f7c6af47d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd68196d0eed43a9a2925fed15955106",
            "placeholder": "​",
            "style": "IPY_MODEL_afdbf4df85ab49ca829705dadd382557",
            "value": "100%"
          }
        },
        "c3d3c9433ab340029c1a85c568ca6a43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6984806ddf14459b849d71086776ab13",
            "max": 38,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a8ac3e5accaf48b7aae70e9f6e5299bf",
            "value": 38
          }
        },
        "0859cf4eaec34979b32886eee6e42f76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8bef0821a13c4d649b09ca275fe67de4",
            "placeholder": "​",
            "style": "IPY_MODEL_2cd9e7df376c4a9e9b30a77903c54ca3",
            "value": " 38/38 [00:48&lt;00:00,  1.25s/it]"
          }
        },
        "be1434fa192444bca0042d452ae68579": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd68196d0eed43a9a2925fed15955106": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afdbf4df85ab49ca829705dadd382557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6984806ddf14459b849d71086776ab13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8ac3e5accaf48b7aae70e9f6e5299bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8bef0821a13c4d649b09ca275fe67de4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cd9e7df376c4a9e9b30a77903c54ca3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}