{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zAFGTy-ycHV0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e9d9efe-77db-4681-f3c6-9e028308f11a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Drive setup\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "276ZYFEEdyuH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm.auto import tqdm\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import sampler\n",
        "from torchvision import datasets\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as T\n",
        "from google.colab.patches import cv2_imshow\n",
        "from torchvision.transforms import ToTensor\n",
        " \n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab.patches import cv2_imshow\n",
        "import cv2\n",
        "import pickle\n",
        "import os\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "from torchvision.io import ImageReadMode, read_image\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.image as mpimg\n",
        "import cv2\n",
        "import pickle\n",
        "import warnings\n",
        "import copy\n",
        "import pickle\n",
        "from google.colab.patches import cv2_imshow\n",
        "import random\n",
        "from sklearn.metrics import average_precision_score\n",
        "import math\n",
        "import torchvision\n",
        "warnings.filterwarnings('ignore')\n",
        "from torch import tensor\n",
        "from torchvision.ops.boxes import box_iou"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnXdW1eFRe_x"
      },
      "source": [
        "## Data Preparation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lnfyhSyFbFmn"
      },
      "outputs": [],
      "source": [
        "with open('/content/drive/MyDrive/SSD detection/images_2.pkl', 'rb') as f:\n",
        "    images = pickle.load(f)\n",
        "with open('/content/drive/MyDrive/SSD detection/labels_2.pkl', 'rb') as f:\n",
        "    labels = pickle.load(f)\n",
        "labels = [[label] for label in labels]\n",
        "with open('/content/drive/MyDrive/SSD detection/coco_person_fire_hydrant_image_3.pickle', 'rb') as f:\n",
        "    images_rest = pickle.load(f)\n",
        "    # images_rest = []\n",
        "for idx in range(len(images_rest)):\n",
        "  images_rest[idx] = np.array(images_rest[idx])\n",
        "with open('/content/drive/MyDrive/SSD detection/coco_person_fire_hydrant_class_labels_3.pickle', 'rb') as f:\n",
        "    labels_rest = pickle.load(f)\n",
        "    # labels_rest = []\n",
        "with open('/content/drive/MyDrive/SSD detection/coco_person_fire_hydrant_annotation_3.pickle', 'rb') as f:\n",
        "    annotations = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "a_gkq3ZEbVAb"
      },
      "outputs": [],
      "source": [
        "random.Random(4).shuffle(images)\n",
        "random.Random(4).shuffle(labels)\n",
        "random.Random(5).shuffle(images_rest)\n",
        "random.Random(5).shuffle(labels_rest)\n",
        "random.Random(5).shuffle(annotations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SxilYopfXT3E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36a556d7-f1ed-46e8-9f31-2018ba8c9a83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "459\n",
            "459\n",
            "438\n",
            "438\n",
            "438\n"
          ]
        }
      ],
      "source": [
        "print(len(images))\n",
        "print(len(labels))\n",
        "print(len(images_rest))\n",
        "print(len(labels_rest))\n",
        "print(len(annotations))\n",
        "# print(len(images_door))\n",
        "# print(len(annotations_door))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Z06Bh0F0Xqd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "743da0b1-f7f0-437f-9a45-abba63a0572d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[137, 91, 226, 217]], [[96, 109, 123, 142]], [[88, 176, 163, 281]], [[67, 1, 163, 138]], [[79, 2, 246, 260]]]\n",
            "[['fire hydrant', 'person', 'person'], ['person', 'person', 'fire hydrant'], ['person', 'person', 'fire hydrant'], ['person', 'person', 'fire hydrant', 'person'], ['fire hydrant', 'person', 'person', 'person']]\n",
            "[[[104, 123, 237, 313], [32, 19, 156, 283], [184, 34, 295, 276]], [[33, 47, 178, 305], [176, 62, 303, 315], [155, 52, 236, 319]], [[234, 24, 303, 278], [115, 50, 145, 85], [15, 243, 76, 319]], [[85, 165, 98, 188], [66, 167, 73, 188], [190, 210, 214, 226], [8, 168, 14, 176]], [[78, 48, 204, 249], [55, 0, 120, 81], [197, 0, 301, 53], [96, 0, 155, 66]]]\n"
          ]
        }
      ],
      "source": [
        "print(labels[:5])\n",
        "print(labels_rest[:5])\n",
        "print(annotations[:5])\n",
        "# print(annotations_door[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmmeHU9uXL4D"
      },
      "source": [
        "Processing images in COCO - Making sure that very small humans are not taken into consideration in the model (harm the results a lot)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "V-VP8ZJBdu_L"
      },
      "outputs": [],
      "source": [
        "images_coco = []\n",
        "labels_coco = []\n",
        "annotations_coco = []\n",
        "val = 0\n",
        "for img in images_rest:\n",
        "    cnt = 0\n",
        "    label_here = []\n",
        "    annotation_here = []\n",
        "    z = 0\n",
        "    if len(annotations[val]) > 4:\n",
        "      val = val + 1\n",
        "      continue\n",
        "    for label in annotations[val]:\n",
        "      # try:\n",
        "        # Checking width and person class\n",
        "        height, width = img.shape[:2]\n",
        "        if ((label[2]-label[0])*(label[3]-label[1]))/(width*height) < 0.005:\n",
        "            continue\n",
        "      # except:\n",
        "        # print(label)\n",
        "        annotation_here.append(label)\n",
        "        label_here.append(labels_rest[val][z])\n",
        "        cnt += 1\n",
        "        z += 1\n",
        "    val += 1\n",
        "    if cnt > 0:\n",
        "        images_coco.append(img)\n",
        "        labels_coco.append(label_here)\n",
        "        annotations_coco.append(annotation_here)\n",
        "\n",
        "# print(len(images_coco))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc6esVs7XXPF"
      },
      "source": [
        "Removing weird images in the dataset (Black & White/malformed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7uDdPBx3L8wZ"
      },
      "outputs": [],
      "source": [
        "def remove_wrong_images(images, labels, annotations = None):\n",
        "    images_new = []\n",
        "    labels_new = []\n",
        "    annotations_new = []\n",
        "    cnt = 0\n",
        "    for cnt in range(len(images)):\n",
        "        # Checking malformed images\n",
        "        if len(images[cnt].shape) < 3 or images[cnt].shape[2] != 3:\n",
        "            continue\n",
        "        images_new.append(images[cnt])\n",
        "        labels_new.append(labels[cnt])\n",
        "        if annotations is not None:\n",
        "            annotations_new.append(annotations[cnt])\n",
        "    return images_new, labels_new, annotations_new\n",
        "\n",
        "images_coco, annotations_coco, labels_coco = remove_wrong_images(images_coco, annotations_coco, labels_coco)\n",
        "# images_door, annotations_door, _ = remove_wrong_images(images_door, annotations_door)\n",
        "images, labels, _ = remove_wrong_images(images, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6maUb7MxpgQz"
      },
      "outputs": [],
      "source": [
        "def image_augmentation_flip_horizontal(images):\n",
        "  original_images = images\n",
        "  new_images = []\n",
        "  for idx in range(len(new_images)):\n",
        "    image = cv2.flip(new_images[idx], 1)#*255\n",
        "    new_images.append(image)\n",
        "    # cv2_imshow(image)\n",
        "  original_images.extend(new_images)\n",
        "  return original_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vsQ6cG6PszoV"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "def image_augmentation_translation(images_list, labels_list, annotations_list):\n",
        "  original_images_list = copy.deepcopy(images_list)\n",
        "  original_labels_list = copy.deepcopy(labels_list)\n",
        "  original_annotations_list = copy.deepcopy(annotations_list)\n",
        "  new_images_list = []\n",
        "  new_labels_list = []\n",
        "  new_annotations_list = []\n",
        "  for img, labels, annotations in zip(images_list, labels_list, annotations_list):\n",
        "    # annotations = [annotations]\n",
        "    height, width = img.shape[:2]\n",
        "    translation_factor = 0.2\n",
        "    max_horizontal = translation_factor*width\n",
        "    max_vertical = translation_factor*height\n",
        "    horizontal_shift = int(random.uniform(-max_horizontal, max_horizontal))\n",
        "    vertical_shift = int(random.uniform(-max_vertical, max_vertical))\n",
        "    # M[0][2] = how much right\n",
        "    # M[1][2] = how much down\n",
        "    M = np.float32([[1, 0, horizontal_shift], [0, 1, vertical_shift]])\n",
        "    new_img = cv2.warpAffine(img, M, (width, height))\n",
        "    # filling blank spaces in image\n",
        "    if horizontal_shift >= 0:\n",
        "      new_img[:,:horizontal_shift] = np.random.rand(320, horizontal_shift, 3)\n",
        "    else:\n",
        "     new_img[:,horizontal_shift:] = np.random.rand(320, -horizontal_shift, 3)\n",
        "    if vertical_shift >= 0:\n",
        "      new_img[:vertical_shift,:] = np.random.rand(vertical_shift, 320,3)\n",
        "    else:\n",
        "      new_img[vertical_shift:,:] = np.random.rand(-vertical_shift, 320,3)\n",
        "\n",
        "    # new_images.append(new_img)\n",
        "    temp_label = []\n",
        "    temp_annotations = []\n",
        "    for label, box in zip(labels, annotations):\n",
        "      x1 = box[0]\n",
        "      y1 = box[1]\n",
        "      x2 = box[2]\n",
        "      y2 = box[3]\n",
        "      x1 = x1 + horizontal_shift\n",
        "      y1 = y1 + vertical_shift\n",
        "      x2 = x2 + horizontal_shift\n",
        "      y2 = y2 + vertical_shift\n",
        "      if x1 >= 0 and x1 < width and y1 >= 0 and y1 < height and x2 >= 0 and x2 < width and y2 >= 0 and y2 < height:\n",
        "        temp_label.append(label) # No cutting\n",
        "        temp_annotations.append([x1, y1, x2, y2]) # No cutting just translation\n",
        "      elif x1 >= width or y1 >= height or x2 < 0 or y2 < 0:\n",
        "        pass # whole out\n",
        "      else: # for cases in which there is partial image cut [if >=30% is image has gone out of bounds then remove else make modifications and add]\n",
        "        # find new coordinates of bbox\n",
        "        initial_area = (x2-x1)*(y2-y1)\n",
        "        if x1 < 0:\n",
        "          x1 = 0\n",
        "        if y1 < 0:\n",
        "          y1 = 0\n",
        "        if x2 >= width:\n",
        "          x2 = width - 1\n",
        "        if y2 >= height:\n",
        "          y2 = height - 1\n",
        "        final_area = (x2-x1)*(y2-y1)\n",
        "        # check if more than 50 % has gone out\n",
        "        if final_area >= 0.7*initial_area:\n",
        "          # accept modified\n",
        "          temp_annotations.append([x1, y1, x2, y2])\n",
        "          temp_label.append(label)\n",
        "        else:\n",
        "          pass\n",
        "        # do something \n",
        "    if len(temp_annotations) > 0:\n",
        "      new_images_list.append(new_img)\n",
        "      new_labels_list.append(temp_label)\n",
        "      new_annotations_list.append(temp_annotations)\n",
        "\n",
        "    # print(horizontal_shift, vertical_shift)\n",
        "    # cv2_imshow(translated)\n",
        "  result_images_list = []\n",
        "  result_labels_list = []\n",
        "  result_annotations_list = []\n",
        "  for img1, img2 in zip(original_images_list, new_images_list):\n",
        "    result_images_list.append(img1)\n",
        "    result_images_list.append(img2)\n",
        "  for label1, label2 in zip(original_labels_list, new_labels_list):\n",
        "    result_labels_list.append(label1)\n",
        "    result_labels_list.append(label2)\n",
        "  for annotation1, annotation2 in zip(original_annotations_list, new_annotations_list):\n",
        "    result_annotations_list.append(annotation1)\n",
        "    result_annotations_list.append(annotation2)\n",
        "\n",
        "  return result_images_list, result_labels_list, result_annotations_list\n",
        "\n",
        "index1 = random.randint(0, len(images_coco))\n",
        "index2 = random.randint(0, len(images_coco))\n",
        "index3 = random.randint(0, len(images_coco))\n",
        "dummy_img_list = [images_coco[index1], images_coco[index2], images_coco[index3]]\n",
        "dummy_class_list = [labels_coco[index1], labels_coco[index2], labels_coco[index3]]\n",
        "dummy_annotations_list = [annotations_coco[index1], annotations_coco[index2], annotations_coco[index3]] \n",
        "\n",
        "# dummy_img_list, dummy_class_list, dummy_annotations_list = image_augmentation_translation(dummy_img_list, dummy_class_list, dummy_annotations_list)\n",
        "# images, dummy_class_list, labels = image_augmentation_translation(images, [[1]]*len(images), labels)\n",
        "# images_coco, labels_coco, annotations_coco = image_augmentation_translation(images_coco, labels_coco, annotations_coco)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "nkw5ibUZ_r-E"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "def img_list_viewer(images_list, labels_list, annotations_list):\n",
        "  for img, labels, annotations in zip(images_list, labels_list, annotations_list):\n",
        "    temp_img = copy.deepcopy(img)\n",
        "    # print(labels, annotations)\n",
        "    for label, annotation in zip(labels, annotations):\n",
        "      # print(label, annotation)\n",
        "      color = None\n",
        "      if label == \"doll\":\n",
        "        color = (1, 0, 0)\n",
        "      elif label == \"fire hydrant\":\n",
        "        color = (0, 1 ,0)\n",
        "      elif label == \"person\":\n",
        "        color = (0, 0 ,1)\n",
        "      # print(annotations)\n",
        "      temp_img = cv2.rectangle(temp_img, (annotation[0], annotation[1]), (annotation[2], annotation[3]), color, 2)\n",
        "    temp_img = temp_img*255\n",
        "    temp_img = np.array(temp_img, dtype=np.uint8)\n",
        "    temp_img = cv2.cvtColor(temp_img, cv2.COLOR_RGB2BGR)\n",
        "    cv2_imshow(temp_img)\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LVOLrhGWzIV"
      },
      "outputs": [],
      "source": [
        "dummy_img, dummy_label, dummy_annotation = image_augmentation_translation(images[:5], [[\"doll\"]]*len(labels[:5]), labels[:5])\n",
        "img_list_viewer(dummy_img, dummy_label, dummy_annotation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fgI1Kg36aBGP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3ab41b2-daa4-49ba-d19f-730eb47cc1af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of doll samples :  459\n",
            "No of person samples :  481\n",
            "No of fire hydrant samples :  305\n"
          ]
        }
      ],
      "source": [
        "# counting no of samples per class\n",
        "print(\"No. of doll samples : \", len(images))\n",
        "count_person = 0\n",
        "count_fire_hydrant = 0\n",
        "flat_labels_coco = [item for sublist in labels_coco for item in sublist]\n",
        "for class_name in flat_labels_coco:\n",
        "  if class_name == \"person\":\n",
        "    count_person = count_person + 1\n",
        "  elif class_name == \"fire hydrant\":\n",
        "    count_fire_hydrant = count_fire_hydrant + 1\n",
        "# print(flat_labels_coco[:5])\n",
        "print(\"No of person samples : \", count_person)\n",
        "print(\"No of fire hydrant samples : \", count_fire_hydrant)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAG_jpgBWv7q"
      },
      "outputs": [],
      "source": [
        "dummy_img, dummy_label, dummy_annotation = image_augmentation_translation(images_rest[:5], labels_rest[:5], annotations[:5])\n",
        "img_list_viewer(dummy_img, dummy_label, dummy_annotation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_3xk-ivjYX8u"
      },
      "outputs": [],
      "source": [
        "images, dummy_list, labels = image_augmentation_translation(images, [[\"doll\"]]*len(labels), labels)\n",
        "images_coco, labels_coco, annotations_coco= image_augmentation_translation(images_coco, labels_coco, annotations_coco)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6OF0kUgvgcOs"
      },
      "outputs": [],
      "source": [
        "# images, dummy_list, labels = image_augmentation_translation(images, [[\"doll\"]]*len(labels), labels)\n",
        "# images_coco, labels_coco, annotations_coco= image_augmentation_translation(images_coco, labels_coco, annotations_coco)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "QvV5jjdxrduM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5ba7aa2-44e2-4da3-c5f6-4dc32c878857"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No. of doll samples :  874\n",
            "No of person samples :  885\n",
            "No of fire hydrant samples :  570\n"
          ]
        }
      ],
      "source": [
        "# counting no of samples per class\n",
        "print(\"No. of doll samples : \", len(images))\n",
        "count_person = 0\n",
        "count_fire_hydrant = 0\n",
        "flat_labels_coco = [item for sublist in labels_coco for item in sublist]\n",
        "for class_name in flat_labels_coco:\n",
        "  if class_name == \"person\":\n",
        "    count_person = count_person + 1\n",
        "  elif class_name == \"fire hydrant\":\n",
        "    count_fire_hydrant = count_fire_hydrant + 1\n",
        "# print(flat_labels_coco[:5])\n",
        "print(\"No of person samples : \", count_person)\n",
        "print(\"No of fire hydrant samples : \", count_fire_hydrant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPcfv1ggXpyB"
      },
      "source": [
        "## Data Augmentation\n",
        "The following image transform classes were defined to deal with bounding boxes in transformations.\n",
        "1. Random Crop (resized to full)\n",
        "2. Perspective Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "hIniPUxx-i0G"
      },
      "outputs": [],
      "source": [
        "# For reference, see PyTorch's implementation of T.RandomResizedCrop\n",
        "class RandomResizedCropWithBox(T.RandomResizedCrop):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(RandomResizedCropWithBox, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, img_data):\n",
        "        img = img_data[0]\n",
        "        boxes = img_data[1]\n",
        "        classes = img_data[2]\n",
        "        i, j, h, w = self.get_params(img, self.scale, self.ratio)\n",
        "        num_boxes = len(boxes)\n",
        "        new_labels = []\n",
        "        new_classes = []\n",
        "\n",
        "        # Creating labels for each bounding box\n",
        "        for val in range(num_boxes):\n",
        "            xmin, ymin, xmax, ymax = boxes[val]\n",
        "            # Checking if it lies inside the cropped image\n",
        "            if xmax <= j or xmin >= j+w or ymax <= i or ymin >= i+h:\n",
        "                continue\n",
        "            x1 = (max(xmin, j)-j)*320/w\n",
        "            x2 = (min(xmax, j+w)-j)*320/w\n",
        "            y1 = (max(ymin, i)-i)*320/h\n",
        "            y2 = (min(ymax, i+h)-i)*320/h\n",
        "            for value in (x1, x2, y1, y2):\n",
        "                if value < 0:\n",
        "                    value = 0\n",
        "                if value >= 320:\n",
        "                    value = 319\n",
        "            new_labels.append([x1, y1, x2, y2])\n",
        "            new_classes.append(classes[val])\n",
        "        new_labels = torch.from_numpy(np.array(new_labels)).int()\n",
        "\n",
        "        # Returns resized image, labels (box co-ordinates) and classes\n",
        "        return [torchvision.transforms.functional.resized_crop(img, i, j, h, w, self.size, self.interpolation), new_labels, new_classes]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqE5_i27RnKv"
      },
      "source": [
        "The custom dataset class which outputs objects according to `idx` - Different ranges give objects from different arrays above.\n",
        "\n",
        "`collate_fn` is used to get the correct format of values from the dataloader (to handle the dictionaries correctly)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "TPEkaOA5Axvf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be8b50b5-5102-47d5-bde3-808975effd0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "699 595\n",
            "175 149\n"
          ]
        }
      ],
      "source": [
        "class DollDataset(Dataset):\n",
        "\n",
        "    # All arrays and values that are part of the Dataset class\n",
        "    def __init__(self, images, labels, images_coco, labels_coco, annotations, xsize, ysize, perspective_prob = 0.5):\n",
        "        self.images = images\n",
        "        self.labels = labels\n",
        "        self.images_rest = images_coco\n",
        "        self.labels_rest = labels_coco\n",
        "        self.annotations = annotations\n",
        "        # self.images_doors = images_doors\n",
        "        # self.labels_doors = labels_doors\n",
        "        self.xsize = xsize\n",
        "        self.ysize = ysize\n",
        "        self.perspective_prob = perspective_prob\n",
        "    \n",
        "    # Combining 3 arrays\n",
        "    def __len__(self):\n",
        "        return len(self.images)+len(self.images_rest)#+len(self.images_doors)\n",
        "\n",
        "    # Crucial function, returns (non-)transformed image with box\n",
        "    def __getitem__(self, idx):\n",
        "        \n",
        "        # Normalize with calculated mean and std dev\n",
        "        trans = T.Compose([T.ToTensor(), T.Resize((self.xsize,self.ysize)), T.Normalize([0.4662, 0.4279, 0.3946], [0.2736, 0.2650, 0.2774])])\n",
        "        # trans = T.Compose([T.ToTensor(), T.Resize((self.xsize,self.ysize))])\n",
        "\n",
        "        # Dealing with individual arrays\n",
        "        z = len(self.images)\n",
        "        z2 = len(self.images_rest)\n",
        "        # print(z, z+z2)\n",
        "        if idx < z:\n",
        "            class_list = [1]\n",
        "        elif idx < z+z2:\n",
        "            class_list = self.labels_rest[idx-z]\n",
        "            for i in range(len(class_list)):\n",
        "                if class_list[i] == 'person':\n",
        "                    class_list[i] = 3\n",
        "                if class_list[i] == 'fire hydrant':\n",
        "                    class_list[i] = 2\n",
        "        # else:\n",
        "        #     class_list = [3]*len(self.labels_doors[idx-z-z2])\n",
        "        try:\n",
        "          if idx < z:\n",
        "              img = self.images[idx]\n",
        "          elif idx < z+z2:\n",
        "              img = self.images_rest[idx-z]\n",
        "        except:\n",
        "          print(\"from img \", idx)\n",
        "        # else:\n",
        "        #     img = self.images_doors[idx-z-z2] \n",
        "        try:\n",
        "          if idx < z:\n",
        "              label = np.array(self.labels[idx])\n",
        "          elif idx < z+z2:\n",
        "              label = np.array(self.annotations[idx-z])\n",
        "          else:\n",
        "            print(\"from label \", idx)\n",
        "        except:\n",
        "          print(\"from label \", idx)\n",
        "        # else:\n",
        "        #     label = np.array(self.labels_doors[idx-z-z2])\n",
        "\n",
        "        # Label resizing\n",
        "        label = torch.from_numpy(label)\n",
        "        y_size, x_size = img.shape[:2]\n",
        "        label[:,1] = label[:,1]*320/y_size\n",
        "        label[:,3] = label[:,3]*320/y_size\n",
        "        label[:,0] = label[:,0]*320/x_size\n",
        "        label[:,2] = label[:,2]*320/x_size\n",
        "        label = label.int()\n",
        "        # print(img)\n",
        "        img = trans(img)\n",
        "        return (img, label, class_list)\n",
        "\n",
        "# Necessary to form a dataloader\n",
        "def collate_fn(data):\n",
        "    dics = []\n",
        "    for x in range(len(data)):\n",
        "        dic = {'image': data[x][0], 'bbox': data[x][1], 'label': torch.tensor(data[x][2])}\n",
        "        dics.append(dic)\n",
        "    return dics\n",
        "\n",
        "print(len(images)*4//5, len(images_coco)*4//5)\n",
        "\n",
        "a = len(images)*4//5\n",
        "b = len(images_coco)*4//5\n",
        "\n",
        "\n",
        "# train dataset\n",
        "random.Random(6).shuffle(images[:a])\n",
        "random.Random(6).shuffle(labels[:a])\n",
        "random.Random(6).shuffle(images_coco[:b])\n",
        "random.Random(6).shuffle(labels_coco[:b])\n",
        "random.Random(6).shuffle(annotations_coco[:b])\n",
        "train_set = DollDataset(images[:a], labels[:a], images_coco[:b], labels_coco[:b], annotations_coco[:b], 320, 320, perspective_prob=-1)\n",
        "\n",
        "print(len(images[a:]), len(images_coco[b:]))\n",
        "# test dataset\n",
        "random.Random(2).shuffle(images[a:])\n",
        "random.Random(2).shuffle(labels[a:])\n",
        "random.Random(2).shuffle(images_coco[b:])\n",
        "random.Random(2).shuffle(labels_coco[b:])\n",
        "random.Random(2).shuffle(annotations_coco[b:])\n",
        "val_set = DollDataset(images[a:], labels[a:], images_coco[b:], labels_coco[b:], annotations_coco[b:], 320, 320, perspective_prob=-1)\n",
        "\n",
        "# print(train_set.__len__())\n",
        "# print(val_set.__len__())\n",
        "\n",
        "# Training and Validation\n",
        "# train_set, _ = torch.utils.data.random_split(doll_set1, [doll_set1.__len__()*1, 0])\n",
        "# val_set, _ = torch.utils.data.random_split(doll_set2, [doll_set2.__len__()*1, 0])\n",
        "train_loader = DataLoader(train_set, batch_size = 16, shuffle = True, collate_fn = collate_fn, drop_last = True)\n",
        "val_loader = DataLoader(val_set, batch_size = 16, shuffle = True, collate_fn = collate_fn, drop_last = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "loB69wuIN2sE"
      },
      "outputs": [],
      "source": [
        "# print(doll_set[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "690iQvHTN-hk"
      },
      "outputs": [],
      "source": [
        "# print(doll_set[7][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "NhUSqFMaW3Iq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0247498-35f3-425d-bea7-dd6307759a93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1294 324\n"
          ]
        }
      ],
      "source": [
        "print(len(train_set), len(val_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "gclqEG9sll6g"
      },
      "outputs": [],
      "source": [
        "# train_set[2430][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "7evOs8zbZOTF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "92f4577f6b7f49efb1cfdcd0ac3fa970",
            "6ef1152dcf4744cba8ccf671a2cef7b4",
            "ebf7a8e183fa4805b119ac1f3b5d60b5",
            "764902e58891449d883c2404b1cfe685",
            "69bf78f941734de8ac1a37f30800b891",
            "2789d3100f9a46d09d137136a511a353",
            "ff5040abe94042e98192596c5dd98ff6",
            "32f916a41c274058a64c269f8ef1bb6f",
            "c80c000f7fd84bcea296d8d2825b7030",
            "5d7453d7e5aa4aed915f794314a0bf86",
            "f6ba6c536221476f8ed98431cd292608"
          ]
        },
        "outputId": "d40d5686-2d43-403b-8f73-0da97778ee82"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/80 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "92f4577f6b7f49efb1cfdcd0ac3fa970"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Testing if the dataloader works\n",
        "for dic in tqdm(train_loader):\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I1RPLfAR_Lj"
      },
      "source": [
        "Testing the working of the dataloader using the ```show``` function for torch tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1zdZh8SlbLy0"
      },
      "outputs": [],
      "source": [
        "# a = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "AwJy5w8eghN2"
      },
      "outputs": [],
      "source": [
        "# import torchvision.transforms.functional as F\n",
        "# import random\n",
        "\n",
        "# random.seed(0)\n",
        "# torch.manual_seed(0)\n",
        "# np.random.seed(0)\n",
        "\n",
        "# # Custom function to display images\n",
        "# def show(imgs):\n",
        "#     if not isinstance(imgs, list):\n",
        "#         imgs = [imgs]\n",
        "#     fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n",
        "#     for i, img in enumerate(imgs):\n",
        "#         img = img.detach()\n",
        "#         img = F.to_pil_image(img)\n",
        "#         axs[0, i].imshow(np.asarray(img))\n",
        "#         axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
        "\n",
        "# from torchvision.transforms.functional import convert_image_dtype\n",
        "# from torchvision.utils import draw_bounding_boxes\n",
        "# plt.rcParams[\"savefig.bbox\"] = 'tight'\n",
        "# # T.Normalize([70.0594, 62.4050, 58.8377], [78.0825, 72.5514, 73.4684]\n",
        "# # Showing one batch of images coming from the data loader\n",
        "# for dic in tqdm(val_loader):\n",
        "#     for x in range(len(dic)):\n",
        "#         z = dic[x]['image']\n",
        "#         z[0] = (z[0]*0.2736+0.4662)*255\n",
        "#         z[1] = (z[1]*0.2650+0.4279)*255\n",
        "#         z[2] = (z[2]*0.2774+0.3946)*255\n",
        "#         # print(type(z))\n",
        "#         # print(z)\n",
        "#         bbox = dic[x]['bbox']\n",
        "#         # print(bbox)\n",
        "#         boxes = []\n",
        "#         boxes.append(torch.tensor([0,0,0,0]))\n",
        "#         for al in range(len(bbox)):\n",
        "#             boxes.append(bbox[al])\n",
        "#         img=draw_bounding_boxes(z.type(torch.uint8), boxes=torch.vstack(boxes), width=4)\n",
        "#         # img = \n",
        "#         show(img)\n",
        "#         break\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjrcFUFozsXj"
      },
      "source": [
        "### Useful functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iou_threshold = 0.5 # for mAP and Confusion matrix\n",
        "nms_iou_threshold = 0.3"
      ],
      "metadata": {
        "id": "mpV20elvbk1a"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def calculate_iou(gt, pr, form='pascal_voc'):\n",
        "#     \"\"\"Calculates the Intersection over Union.\n",
        "\n",
        "#     Arguments:\n",
        "#         gt: (torch.Tensor[N,4]) coordinates of the ground-truth boxes\n",
        "#         pr: (torch.Tensor[M,4]) coordinates of the prdicted boxes\n",
        "#         form: (str) gt/pred coordinates format\n",
        "#             - pascal_voc: [xmin, ymin, xmax, ymax]\n",
        "#             - coco: [xmin, ymin, w, h]\n",
        "#     Returns:\n",
        "#         iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
        "#         IoU values for every element in boxes1 and boxes2\n",
        "#     \"\"\"\n",
        "#     if form == 'coco':\n",
        "#         gt = gt.clone()\n",
        "#         pr = pr.clone()\n",
        "\n",
        "#         gt[:,2] = gt[:,0] + gt[:,2]\n",
        "#         gt[:,3] = gt[:,1] + gt[:,3]\n",
        "#         pr[:,2] = pr[:,0] + pr[:,2]\n",
        "#         pr[:,3] = pr[:,1] + pr[:,3]\n",
        "\n",
        "#     # gt = align_coordinates(gt)\n",
        "#     # pr = align_coordinates(pr)\n",
        "    \n",
        "#     return box_iou(gt,pr)\n",
        "# iou_mat = calculate_iou(targs,preds,form='coco');\n",
        "# gt_count, pr_count = iou_mat.shape\n",
        "# thresh = 0.5\n",
        "# iou_mat = iou_mat.where(iou_mat>thresh,tensor(0.))\n",
        "# def get_mappings(iou_mat):\n",
        "#     mappings = torch.zeros_like(iou_mat)\n",
        "#     #first mapping (max iou for first pred_box)\n",
        "#     if not iou_mat[:,0].eq(0.).all():\n",
        "#         # if not a zero column\n",
        "#         mappings[iou_mat[:,0].argsort()[-1],0] = 1\n",
        "\n",
        "#     for pr_idx in range(1,pr_count):\n",
        "#         # Sum of all the previous mapping columns will let \n",
        "#         # us know which gt-boxes are already assigned\n",
        "#         not_assigned = torch.logical_not(mappings[:,:pr_idx].sum(1)).long()\n",
        "\n",
        "#         # Considering unassigned gt-boxes for further evaluation \n",
        "#         targets = not_assigned * iou_mat[:,pr_idx]\n",
        "\n",
        "#         # If no gt-box satisfy the previous conditions\n",
        "#         # for the current pred-box, ignore it (False Positive)\n",
        "#         if targets.eq(0).all():\n",
        "#             continue\n",
        "\n",
        "#         # max-iou from current column after all the filtering\n",
        "#         # will be the pivot element for mapping\n",
        "#         pivot = targets.argsort()[-1]\n",
        "#         mappings[pivot,pr_idx] = 1\n",
        "#     return mappings\n",
        "\n",
        "# mappings = get_mappings(iou_mat)\n",
        "# tp = mappings.sum(); tp\n",
        "# fp = mappings.sum(0).eq(0).sum(); fp\n",
        "# fn = mappings.sum(1).eq(0).sum(); fn\n",
        "# def calculate_map(gt_boxes,pr_boxes,scores,thresh=0.5,form='pascal_voc'):\n",
        "#     # sorting\n",
        "#     pr_boxes = pr_boxes[scores.argsort().flip(-1)]\n",
        "#     iou_mat = calculate_iou(gt_boxes,pr_boxes,form)\n",
        "#     gt_count, pr_count = iou_mat.shape\n",
        "    \n",
        "#     # thresholding\n",
        "#     iou_mat = iou_mat.where(iou_mat>thresh,tensor(0.))\n",
        "    \n",
        "#     mappings = get_mappings(iou_mat)\n",
        "    \n",
        "#     # mAP calculation\n",
        "#     tp = mappings.sum()\n",
        "#     fp = mappings.sum(0).eq(0).sum()\n",
        "#     fn = mappings.sum(1).eq(0).sum()\n",
        "#     mAP = tp / (tp+fp+fn)\n",
        "    \n",
        "#     return mAP"
      ],
      "metadata": {
        "id": "5avO-yYytdbn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "rrl1qNhL1D6j"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"corners\"):\n",
        "    \"\"\"\n",
        "    Calculates intersection over union\n",
        "    Parameters:\n",
        "        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
        "        boxes_labels (tensor): Correct Labels of Boxes (BATCH_SIZE, 4)\n",
        "        box_format (str): midpoint/corners, if boxes (x,y,w,h) or (x1,y1,x2,y2)\n",
        "    Returns:\n",
        "        tensor: Intersection over union for all examples\n",
        "    \"\"\"\n",
        "\n",
        "    # Slicing idx:idx+1 in order to keep tensor dimensionality\n",
        "    # Doing ... in indexing if there would be additional dimensions\n",
        "    # Like for Yolo algorithm which would have (N, S, S, 4) in shape\n",
        "    if box_format == \"midpoint\":\n",
        "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
        "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
        "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
        "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
        "    elif box_format == \"corners\":\n",
        "        box1_x1 = boxes_preds[..., 0:1]\n",
        "        # print(\"box1_x1 : \", box1_x1)\n",
        "        box1_y1 = boxes_preds[..., 1:2]\n",
        "        box1_x2 = boxes_preds[..., 2:3]\n",
        "        box1_y2 = boxes_preds[..., 3:4]\n",
        "        box2_x1 = boxes_labels[..., 0:1]\n",
        "        box2_y1 = boxes_labels[..., 1:2]\n",
        "        box2_x2 = boxes_labels[..., 2:3]\n",
        "        box2_y2 = boxes_labels[..., 3:4]\n",
        "\n",
        "    # print(\"box format : \", box_format)\n",
        "    # print(box1_x1)\n",
        "    # print(box2_x1)\n",
        "    x1 = torch.max(box1_x1, box2_x1)\n",
        "    y1 = torch.max(box1_y1, box2_y1)\n",
        "    x2 = torch.min(box1_x2, box2_x2)\n",
        "    y2 = torch.min(box1_y2, box2_y2)\n",
        "\n",
        "    # Need clamp(0) in case they do not intersect, then we want intersection to be 0\n",
        "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
        "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
        "\n",
        "    return intersection / (box1_area + box2_area - intersection + 1e-6)\n",
        "\n",
        "def mean_average_precision(\n",
        "    ls, targets_model, iou_threshold=0.5, box_format=\"corners\", num_classes=3\n",
        "):\n",
        "    # print(\"box_format in mAP 1 : \", box_format)\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Calculates mean average precision \n",
        "    Parameters:\n",
        "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
        "        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n",
        "        true_boxes (list): Similar as pred_boxes except all the correct ones \n",
        "        iou_threshold (float): threshold where predicted bboxes is correct\n",
        "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
        "        num_classes (int): number of classes\n",
        "    Returns:\n",
        "        float: mAP value across all classes given a specific IoU threshold \n",
        "    \"\"\"\n",
        "    pred_boxes = []\n",
        "    for idx, ls_for_each_img in enumerate(ls):\n",
        "      boxes = ls_for_each_img[\"boxes\"]\n",
        "      scores = ls_for_each_img[\"scores\"]\n",
        "      labels = ls_for_each_img[\"labels\"]\n",
        "      boxes = boxes.cpu().detach().numpy()\n",
        "      scores = scores.cpu().detach().numpy()\n",
        "      labels = labels.cpu().detach().numpy()\n",
        "      for single_detection in zip(boxes, scores, labels):\n",
        "        # print(single_detection)\n",
        "        pred_boxes.append([idx, single_detection[2], single_detection[1], single_detection[0][0], single_detection[0][1], single_detection[0][2], single_detection[0][3]])\n",
        "        # break\n",
        "      # break\n",
        "    true_boxes = []\n",
        "    for idx, target_for_each_img in enumerate(targets_model):\n",
        "      # print(target_for_each_img)\n",
        "      boxes = target_for_each_img[\"boxes\"]\n",
        "      labels = target_for_each_img[\"labels\"]\n",
        "      boxes = boxes.cpu().detach().numpy()\n",
        "      labels = labels.cpu().detach().numpy()\n",
        "      for single_detection in zip(boxes, labels):\n",
        "        # print(single_detection)\n",
        "        true_boxes.append([idx, single_detection[1], 1, single_detection[0][0], single_detection[0][1], single_detection[0][2], single_detection[0][3]])\n",
        "\n",
        "    # list storing all AP for respective classes\n",
        "    average_precisions = []\n",
        "\n",
        "    # used for numerical stability later on\n",
        "    epsilon = 1e-6\n",
        "    # print(\"box_format in mAP 2 : \", box_format)\n",
        "    for c in range(1, num_classes+1):\n",
        "        detections = []\n",
        "        ground_truths = []\n",
        "\n",
        "        # Go through all predictions and targets,\n",
        "        # and only add the ones that belong to the\n",
        "        # current class c\n",
        "        for detection in pred_boxes:\n",
        "            if detection[1] == c:\n",
        "                detections.append(detection)\n",
        "\n",
        "        for true_box in true_boxes:\n",
        "            if true_box[1] == c:\n",
        "                ground_truths.append(true_box)\n",
        "\n",
        "        # find the amount of bboxes for each training example\n",
        "        # Counter here finds how many ground truth bboxes we get\n",
        "        # for each training example, so let's say img 0 has 3,\n",
        "        # img 1 has 5 then we will obtain a dictionary with:\n",
        "        # amount_bboxes = {0:3, 1:5}\n",
        "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
        "\n",
        "        # We then go through each key, val in this dictionary\n",
        "        # and convert to the following (w.r.t same example):\n",
        "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
        "        for key, val in amount_bboxes.items():\n",
        "            amount_bboxes[key] = torch.zeros(val)\n",
        "\n",
        "        # sort by box probabilities which is index 2\n",
        "        detections.sort(key=lambda x: x[2], reverse=True)\n",
        "        TP = torch.zeros((len(detections)))\n",
        "        FP = torch.zeros((len(detections)))\n",
        "        total_true_bboxes = len(ground_truths)\n",
        "        \n",
        "        # If none exists for this class then we can safely skip\n",
        "        if total_true_bboxes == 0:\n",
        "            continue\n",
        "        # print(\"box_format in mAP 3 : \", box_format)\n",
        "        for detection_idx, detection in enumerate(detections):\n",
        "            # Only take out the ground_truths that have the same\n",
        "            # training idx as detection\n",
        "            ground_truth_img = [\n",
        "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
        "            ]\n",
        "\n",
        "            num_gts = len(ground_truth_img)\n",
        "            best_iou = 0\n",
        "            # print(\"box_format in mAP : \", box_format)\n",
        "\n",
        "            for idx, gt in enumerate(ground_truth_img):\n",
        "                iou = intersection_over_union(\n",
        "                    torch.tensor(detection[3:]),\n",
        "                    torch.tensor(gt[3:]),\n",
        "                    box_format=box_format,\n",
        "                )\n",
        "\n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    best_gt_idx = idx\n",
        "\n",
        "            if best_iou > iou_threshold:\n",
        "                # only detect ground truth detection once\n",
        "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
        "                    # true positive and add this bounding box to seen\n",
        "                    TP[detection_idx] = 1\n",
        "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
        "                else:\n",
        "                    FP[detection_idx] = 1\n",
        "\n",
        "            # if IOU is lower then the detection is a false positive\n",
        "            else:\n",
        "                FP[detection_idx] = 1\n",
        "\n",
        "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
        "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
        "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
        "        precisions = TP_cumsum / (TP_cumsum + FP_cumsum + epsilon)\n",
        "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
        "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
        "        # torch.trapz for numerical integration\n",
        "        average_precisions.append(torch.trapz(precisions, recalls))\n",
        "\n",
        "    return sum(average_precisions) / len(average_precisions), average_precisions"
      ],
      "metadata": {
        "id": "OVr9mc7Vt2eu"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "OAnAM__Kzw-_"
      },
      "outputs": [],
      "source": [
        "# Calculates IOU\n",
        "def iou(box1, box2):\n",
        "    # box1 = list(map(lambda x: int(x), box1))\n",
        "    # box2 = list(map(lambda x: int(x), box2))\n",
        "    a1 = (box1[2]-box1[0])*(box1[3]-box1[1])\n",
        "    a2 = (box2[2]-box2[0])*(box2[3]-box2[1])\n",
        "    inter = max(0, min(box1[2], box2[2]) - max(box1[0], box2[0])) * max(0, min(box1[3], box2[3]) - max(box1[1], box2[1]))\n",
        "    return inter/(a1 + a2 - inter)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "3nAVYgX9R8kB"
      },
      "outputs": [],
      "source": [
        "def batch_nms_confidence_filter(ls, hard = True, conf_threhold = 0.5):\n",
        "  for idx in range(len(ls)):\n",
        "    \n",
        "    if hard: # does not give confidence filtered output\n",
        "      ls_index = torchvision.ops.batched_nms(boxes = ls[idx][\"boxes\"], scores = ls[idx][\"scores\"], idxs = ls[idx][\"labels\"], iou_threshold = nms_iou_threshold)\n",
        "      confidence_filtered_ls_index = []\n",
        "      for index in ls_index:\n",
        "        if ls[idx][\"scores\"][index] > conf_threhold:\n",
        "          confidence_filtered_ls_index.append(index)\n",
        "      ls_index = confidence_filtered_ls_index\n",
        "    # else:    # gives confidence filtered output\n",
        "    #   ls_index = soft_nms_pytorch(ls[idx][\"boxes\"], ls[idx][\"scores\"], thresh = conf_threhold) \n",
        "    \n",
        "    temp_ls = {\"boxes\":[], \"scores\":[], \"labels\":[]}\n",
        "    \n",
        "    for index in ls_index:\n",
        "      temp_ls[\"boxes\"].append(ls[idx][\"boxes\"][index])\n",
        "      temp_ls[\"scores\"].append(ls[idx][\"scores\"][index])\n",
        "      temp_ls[\"labels\"].append(ls[idx][\"labels\"][index])\n",
        "    if len(temp_ls[\"scores\"]) > 0:\n",
        "      temp_ls[\"boxes\"] = torch.stack(temp_ls[\"boxes\"])\n",
        "      temp_ls[\"scores\"] = torch.tensor(temp_ls[\"scores\"])\n",
        "      temp_ls[\"labels\"] = torch.tensor(temp_ls[\"labels\"])\n",
        "    else:\n",
        "      temp_ls[\"boxes\"] = torch.tensor([[]])\n",
        "      temp_ls[\"scores\"] = torch.tensor([])\n",
        "      temp_ls[\"labels\"] = torch.tensor([])\n",
        "    ls[idx] = temp_ls\n",
        "  return ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def confusion_matrix(ls, targets_model, TP, FP, FN, TN, iou_threshold = iou_threshold):\n",
        "      for x in range(len(ls)):\n",
        "        predicted_scores = ls[x][\"scores\"].detach().to(\"cpu\").numpy() \n",
        "        predicted_labels = ls[x][\"labels\"].detach().to(\"cpu\").numpy() \n",
        "        predicted_boxes = ls[x][\"boxes\"].detach().to(\"cpu\").numpy()\n",
        "        # ground_scores = targets_model[x][\"scores\"].detach().to(\"cpu\").numpy() \n",
        "        ground_labels = targets_model[x][\"labels\"].detach().to(\"cpu\").numpy() \n",
        "        ground_boxes =  targets_model[x][\"boxes\"].detach().to(\"cpu\").numpy() \n",
        "        for index, (predicted_box, predicted_label) in enumerate(zip(predicted_boxes, predicted_labels)):\n",
        "          for idx, (ground_box, ground_label) in enumerate(zip(ground_boxes, ground_labels)):\n",
        "            if predicted_label == ground_label and iou(predicted_box, ground_box) > iou_threshold:\n",
        "              TP += 1\n",
        "              ground_labels[idx] = -1\n",
        "              predicted_labels[index] = -2\n",
        "              break\n",
        "\n",
        "        not_counted = 0\n",
        "        for label in ground_labels:\n",
        "          if label != -1:\n",
        "            not_counted += 1\n",
        "        FN += not_counted\n",
        "\n",
        "        false_counted = 0\n",
        "        for label in predicted_labels:\n",
        "          if label != -2:\n",
        "            false_counted += 1\n",
        "        FP += false_counted\n",
        "      return (TP, FP, FN, TN)"
      ],
      "metadata": {
        "id": "WXgVgy1HgnCn"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEOS19dgznx6"
      },
      "source": [
        "## model loading"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# my_model = torch.load(\"/content/drive/MyDrive/SSD detection/SSD detection_3classes_4Xdata_augmented.pth\")\n",
        "# model = my_model\n",
        "# model.eval()"
      ],
      "metadata": {
        "id": "0bDeUTry2LcP"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "5BKd0964fxx2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1700da9-9354-45fd-94a7-75e36dee26a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SSD(\n",
              "  (backbone): SSDLiteFeatureExtractorMobileNet(\n",
              "    (features): Sequential(\n",
              "      (0): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=16, bias=False)\n",
              "              (1): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(16, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
              "              (1): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (2): ConvNormActivation(\n",
              "              (0): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (3): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(72, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=72, bias=False)\n",
              "              (1): BatchNorm2d(72, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (2): ConvNormActivation(\n",
              "              (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(24, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (4): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(72, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(72, 72, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=72, bias=False)\n",
              "              (1): BatchNorm2d(72, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(24, 72, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): ReLU()\n",
              "              (scale_activation): Hardsigmoid()\n",
              "            )\n",
              "            (3): ConvNormActivation(\n",
              "              (0): Conv2d(72, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(40, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (5): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(120, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
              "              (1): BatchNorm2d(120, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): ReLU()\n",
              "              (scale_activation): Hardsigmoid()\n",
              "            )\n",
              "            (3): ConvNormActivation(\n",
              "              (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(40, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (6): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(120, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
              "              (1): BatchNorm2d(120, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): ReLU(inplace=True)\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): ReLU()\n",
              "              (scale_activation): Hardsigmoid()\n",
              "            )\n",
              "            (3): ConvNormActivation(\n",
              "              (0): Conv2d(120, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(40, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (7): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(240, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=240, bias=False)\n",
              "              (1): BatchNorm2d(240, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (2): ConvNormActivation(\n",
              "              (0): Conv2d(240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (8): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(80, 200, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(200, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
              "              (1): BatchNorm2d(200, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (2): ConvNormActivation(\n",
              "              (0): Conv2d(200, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (9): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(184, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
              "              (1): BatchNorm2d(184, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (2): ConvNormActivation(\n",
              "              (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (10): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(80, 184, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(184, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(184, 184, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=184, bias=False)\n",
              "              (1): BatchNorm2d(184, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (2): ConvNormActivation(\n",
              "              (0): Conv2d(184, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (11): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): ReLU()\n",
              "              (scale_activation): Hardsigmoid()\n",
              "            )\n",
              "            (3): ConvNormActivation(\n",
              "              (0): Conv2d(480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(112, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (12): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
              "              (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): ReLU()\n",
              "              (scale_activation): Hardsigmoid()\n",
              "            )\n",
              "            (3): ConvNormActivation(\n",
              "              (0): Conv2d(672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(112, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (13): ConvNormActivation(\n",
              "          (0): Conv2d(112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): Sequential(\n",
              "          (1): ConvNormActivation(\n",
              "            (0): Conv2d(672, 672, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=672, bias=False)\n",
              "            (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): Hardswish()\n",
              "          )\n",
              "          (2): SqueezeExcitation(\n",
              "            (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "            (fc1): Conv2d(672, 168, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (fc2): Conv2d(168, 672, kernel_size=(1, 1), stride=(1, 1))\n",
              "            (activation): ReLU()\n",
              "            (scale_activation): Hardsigmoid()\n",
              "          )\n",
              "          (3): ConvNormActivation(\n",
              "            (0): Conv2d(672, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "            (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          )\n",
              "        )\n",
              "        (1): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
              "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): ReLU()\n",
              "              (scale_activation): Hardsigmoid()\n",
              "            )\n",
              "            (3): ConvNormActivation(\n",
              "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (2): InvertedResidual(\n",
              "          (block): Sequential(\n",
              "            (0): ConvNormActivation(\n",
              "              (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (1): ConvNormActivation(\n",
              "              (0): Conv2d(480, 480, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=480, bias=False)\n",
              "              (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "              (2): Hardswish()\n",
              "            )\n",
              "            (2): SqueezeExcitation(\n",
              "              (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "              (fc1): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (fc2): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
              "              (activation): ReLU()\n",
              "              (scale_activation): Hardsigmoid()\n",
              "            )\n",
              "            (3): ConvNormActivation(\n",
              "              (0): Conv2d(480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "              (1): BatchNorm2d(80, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (3): ConvNormActivation(\n",
              "          (0): Conv2d(80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (extra): ModuleList(\n",
              "      (0): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(480, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)\n",
              "          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): ConvNormActivation(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
              "          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): ConvNormActivation(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (2): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=128, bias=False)\n",
              "          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): ConvNormActivation(\n",
              "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "      )\n",
              "      (3): Sequential(\n",
              "        (0): ConvNormActivation(\n",
              "          (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (1): ConvNormActivation(\n",
              "          (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=64, bias=False)\n",
              "          (1): BatchNorm2d(64, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "        (2): ConvNormActivation(\n",
              "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "          (2): ReLU6(inplace=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (anchor_generator): DefaultBoxGenerator(aspect_ratios=[[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]], clip=True, scales=[0.2, 0.35, 0.5, 0.65, 0.8, 0.95, 1.0], steps=None)\n",
              "  (head): SSDLiteHead(\n",
              "    (classification_head): SSDLiteClassificationHead(\n",
              "      (module_list): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): ConvNormActivation(\n",
              "            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
              "            (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(672, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): ConvNormActivation(\n",
              "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "            (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(480, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): Sequential(\n",
              "          (0): ConvNormActivation(\n",
              "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
              "            (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (3): Sequential(\n",
              "          (0): ConvNormActivation(\n",
              "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "            (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(256, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (4): Sequential(\n",
              "          (0): ConvNormActivation(\n",
              "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "            (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(256, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (5): Sequential(\n",
              "          (0): ConvNormActivation(\n",
              "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
              "            (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(128, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (regression_head): SSDLiteRegressionHead(\n",
              "      (module_list): ModuleList(\n",
              "        (0): Sequential(\n",
              "          (0): ConvNormActivation(\n",
              "            (0): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672, bias=False)\n",
              "            (1): BatchNorm2d(672, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(672, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (1): Sequential(\n",
              "          (0): ConvNormActivation(\n",
              "            (0): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480, bias=False)\n",
              "            (1): BatchNorm2d(480, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(480, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (2): Sequential(\n",
              "          (0): ConvNormActivation(\n",
              "            (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)\n",
              "            (1): BatchNorm2d(512, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (3): Sequential(\n",
              "          (0): ConvNormActivation(\n",
              "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "            (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(256, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (4): Sequential(\n",
              "          (0): ConvNormActivation(\n",
              "            (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)\n",
              "            (1): BatchNorm2d(256, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(256, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "        (5): Sequential(\n",
              "          (0): ConvNormActivation(\n",
              "            (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128, bias=False)\n",
              "            (1): BatchNorm2d(128, eps=0.001, momentum=0.03, affine=True, track_running_stats=True)\n",
              "            (2): ReLU6(inplace=True)\n",
              "          )\n",
              "          (1): Conv2d(128, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (transform): GeneralizedRCNNTransform(\n",
              "      Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
              "      Resize(min_size=(320,), max_size=320, mode='bilinear')\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "my_model = torch.load(\"/content/drive/MyDrive/SSD detection/models/SSD detection_3classes_2Xdata_augmented.pth\")\n",
        "model = my_model\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "x1qCJb9GW_2Y"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "5WOfv4PxaIWm"
      },
      "outputs": [],
      "source": [
        "# vaibhav_model = torch.load(\"/content/vaibhav_model_dump.pth\")\n",
        "# model = vaibhav_model\n",
        "# model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVIZ2q7Bzi0d"
      },
      "source": [
        "## mAP"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_target = None"
      ],
      "metadata": {
        "id": "tjieLm6e9sX1"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "BTzISp5esVw8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414,
          "referenced_widgets": [
            "773b6776f3db4a6b978753faa435fbc1",
            "ffcac1f3f796444a93d3683b08babe5e",
            "1778dc3b43494d1182b8446652f56458",
            "ca198789fa584aa3a252efa42b831ff5",
            "8bad99ec6f90496898568a5ab4920685",
            "b2f3577c08a14a24b64f19dea7281f47",
            "2a789d8763874f06937928cdfbeaef17",
            "37428a272dc54301b155dba261808713",
            "b450178b4d874b80b3de0e5214af8589",
            "e9e1f1e887d1471398e43d938a3cdfa5",
            "85142b00eebf4e0cad35c703ebda97a5"
          ]
        },
        "outputId": "7acc776d-89ee-4b24-9b6f-8653920bdb55"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "773b6776f3db4a6b978753faa435fbc1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor(1.0000), tensor(0.6375), tensor(0.5492)]\n",
            "[tensor(0.8000), tensor(0.3917), tensor(0.5714)]\n",
            "[tensor(0.9091), tensor(0.4000), tensor(0.6771)]\n",
            "[tensor(0.8889), tensor(0.3083), tensor(0.3775)]\n",
            "[tensor(0.7629), tensor(0.3333), tensor(0.5893)]\n",
            "[tensor(1.0000), tensor(0.9000), tensor(0.6364)]\n",
            "[tensor(0.8889), tensor(0.0556), tensor(0.6667)]\n",
            "[tensor(0.7778), tensor(0.5000), tensor(0.5393)]\n",
            "[tensor(0.9000), tensor(0.5000), tensor(0.5000)]\n",
            "[tensor(0.8758), tensor(0.6000), tensor(0.3861)]\n",
            "[tensor(0.8571), tensor(0.8333), tensor(0.4545)]\n",
            "[tensor(0.6250), tensor(0.6143), tensor(0.4973)]\n",
            "[tensor(0.7273), tensor(0.7500), tensor(0.7500)]\n",
            "[tensor(0.7778), tensor(0.3333), tensor(0.4000)]\n",
            "[tensor(0.8750), tensor(0.6133), tensor(0.4286)]\n",
            "[tensor(1.0000), tensor(1.0000), tensor(1.0000)]\n",
            "[tensor(0.8333), tensor(0.4854), tensor(0.4167)]\n",
            "[tensor(1.0000), tensor(0.3958), tensor(0.5714)]\n",
            "[tensor(0.8889), tensor(0.4000), tensor(0.9441)]\n",
            "[tensor(1.0000), tensor(0.5000), tensor(0.8609)]\n",
            "Overall map :0.6626021265983582\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "map_class_all = 0\n",
        "map_class1 = 0\n",
        "map_class2 = 0\n",
        "map_class3 = 0\n",
        "updates = 0\n",
        "for dic in tqdm(val_loader):\n",
        "    images_model = []\n",
        "    targets_model = []\n",
        "    for x in range(len(dic)):\n",
        "        images_model.append(dic[x]['image'].float())\n",
        "        dictionary = {'boxes': dic[x]['bbox'], 'labels': dic[x]['label']}\n",
        "        targets_model.append(dictionary)\n",
        "    ls = model.forward(images_model)\n",
        "    a = ls\n",
        "    b = targets_model\n",
        "    ls = batch_nms_confidence_filter(ls, hard = True)\n",
        "    temp_target = targets_model\n",
        "    # a = ls\n",
        "    # print(ls)\n",
        "    # break\n",
        "    # map_class1 += mean_average_precision(ls, targets_model, iou_threshold, 1,1)\n",
        "    # map_class2 += mean_average_precision(ls, targets_model, iou_threshold, 2,2)\n",
        "    # map_class3 += mean_average_precision(ls, targets_model, iou_threshold, 3,3)\n",
        "    mAP = mean_average_precision(ls, targets_model, iou_threshold)\n",
        "    map_class_all += mAP[0]\n",
        "    print(mAP[1])\n",
        "    # confusion_matrix\n",
        "    # map = map + c\n",
        "    updates += 1\n",
        "# print(\"For class 1 map:{} | For class 2 map : {} | For class 3 map : {}\".format(map_class1/updates, map_class2/updates, map_class3/updates))\n",
        "print(\"Overall map :{}\".format(map_class_all/updates))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOEGt-Mlzev1"
      },
      "source": [
        "## CONFUSION MATRIX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "ra9mHJjpS7rb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "669712cd1c694c308a6cb0180fbaf094",
            "83bdb133af6f437786ed0c661d5ee71b",
            "5bcb88b1f6684343b0df7348c80d5509",
            "222bd5bd3928497faf7fda38566d1948",
            "d8d032db23a34d7f80fb8c894a99daac",
            "0fea0b75510c49e88c8b9168f1bcb33e",
            "c8f63152d9cf4cd1bc9dd85f7fed416d",
            "10c5cf27b83a4814a7094067670532f5",
            "c048b99e65db46ef92ea2adc8b3fd65a",
            "65db7d7fc71d4102a783d2803a8f5cb7",
            "cc8c27343f30462694ac19ac0f529cda"
          ]
        },
        "outputId": "abf73c33-d7ec-4c4d-e9f6-320e2bbbea7d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/20 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "669712cd1c694c308a6cb0180fbaf094"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TP = 313 | FP = 48\n",
            "-----------------\n",
            "FN = 129 | TN = Not defined\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "map = 0\n",
        "updates = 0\n",
        "val_loader_idx = 0\n",
        "\n",
        "TP = 0\n",
        "FP = 0\n",
        "FN = 0\n",
        "TN = 0\n",
        "\n",
        "for dic in tqdm(val_loader):\n",
        "    images_model = []\n",
        "    targets_model = []\n",
        "    for x in range(len(dic)):\n",
        "        images_model.append(dic[x]['image'].float())\n",
        "        dictionary = {'boxes': dic[x]['bbox'], 'labels': dic[x]['label']}\n",
        "        targets_model.append(dictionary)\n",
        "    ls = model.forward(images_model)\n",
        "    ls = batch_nms_confidence_filter(ls, hard = True)\n",
        "    TP, FP, FN, TN = confusion_matrix(ls, targets_model, TP, FP, FN, TN, iou_threshold)\n",
        "\n",
        "print(\"TP = {} | FP = {}\".format(TP, FP))\n",
        "print(\"-----------------\")\n",
        "print(\"FN = {} | TN = {}\".format(FN, \"Not defined\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "OIcPWHlYGw1Q"
      },
      "outputs": [],
      "source": [
        "# model.eval()\n",
        "# map = 0\n",
        "# updates = 0\n",
        "# val_loader_idx = 0\n",
        "# for dic in tqdm(val_loader):\n",
        "#     images_model = []\n",
        "#     targets_model = []\n",
        "#     for x in range(len(dic)):\n",
        "#         images_model.append(dic[x]['image'].float())\n",
        "#         dictionary = {'boxes': dic[x]['bbox'], 'labels': dic[x]['label']}\n",
        "#         targets_model.append(dictionary)\n",
        "#     ls = model.forward(images_model)\n",
        "#     ls = batch_nms_confidence_filter(ls, hard = True)\n",
        "#     for x in range(len(ls)):\n",
        "#       z = dic[x]['image']\n",
        "#       z[0] = (z[0]*0.2736+0.4662)*255\n",
        "#       z[1] = (z[1]*0.2650+0.4279)*255\n",
        "#       z[2] = (z[2]*0.2774+0.3946)*255\n",
        "#       z = z.cpu().detach().numpy()\n",
        "#       temp_img = copy.deepcopy(z)\n",
        "#       temp_img = np.array(temp_img, dtype='uint8')\n",
        "#       temp_img1 = np.zeros((320, 320, 3), dtype=\"uint8\")\n",
        "#       for i in range(320):\n",
        "#         for j in range(320):\n",
        "#           temp_img1[j][i] = (temp_img[0][j][i], temp_img[1][j][i], temp_img[2][j][i])\n",
        "#       # print(temp_img)\n",
        "#       temp_img1 = cv2.cvtColor(temp_img1, cv2.COLOR_BGR2RGB)\n",
        "#       scores = ls[x][\"scores\"].detach().to(\"cpu\").numpy() \n",
        "#       labels = ls[x][\"labels\"].detach().to(\"cpu\").numpy() \n",
        "#       boxes = ls[x][\"boxes\"].detach().to(\"cpu\").numpy()\n",
        "#       boxes = np.array(boxes, dtype='int')\n",
        "#       for label, box in zip(labels, boxes):\n",
        "#         color = None\n",
        "#         if label == 1:\n",
        "#           color = (255, 0, 0)\n",
        "#         elif label == 2:\n",
        "#           color = (0, 255, 0)\n",
        "#         elif label == 3:\n",
        "#           color = (0, 0, 255)\n",
        "#         # print(temp_img)\n",
        "#         # print(box)\n",
        "#         temp_img1 = cv2.rectangle(temp_img1, (box[0], box[1]), (box[2], box[3]), color, 2)\n",
        "#       cv2_imshow(temp_img1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiiEIJLOAzVo",
        "outputId": "98e0af9a-fdad-4df5-df00-aa2ca24adf9a"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'boxes': tensor([[ 74.7205,  19.5787, 172.8983, 144.3649]], grad_fn=<StackBackward0>),\n",
              "  'labels': tensor([1]),\n",
              "  'scores': tensor([0.9836])},\n",
              " {'boxes': tensor([[ 90.5548, 118.2727, 160.5769, 226.7522]], grad_fn=<StackBackward0>),\n",
              "  'labels': tensor([1]),\n",
              "  'scores': tensor([0.9990])},\n",
              " {'boxes': tensor([[129.4462, 123.8077, 248.7479, 273.1547]], grad_fn=<StackBackward0>),\n",
              "  'labels': tensor([1]),\n",
              "  'scores': tensor([0.9949])},\n",
              " {'boxes': tensor([[190.4005, 211.5965, 271.9470, 307.7177]], grad_fn=<StackBackward0>),\n",
              "  'labels': tensor([1]),\n",
              "  'scores': tensor([0.9932])},\n",
              " {'boxes': tensor([[139.2632,  67.3201, 236.9120, 202.3436]], grad_fn=<StackBackward0>),\n",
              "  'labels': tensor([1]),\n",
              "  'scores': tensor([0.9980])},\n",
              " {'boxes': tensor([[ 68.3663,  30.1075, 170.2031, 160.7437]], grad_fn=<StackBackward0>),\n",
              "  'labels': tensor([1]),\n",
              "  'scores': tensor([0.9531])},\n",
              " {'boxes': tensor([[255.0121,  86.8395, 286.9255, 196.0137],\n",
              "          [143.9222,  65.3582, 195.3079, 175.6862]], grad_fn=<StackBackward0>),\n",
              "  'labels': tensor([3, 3]),\n",
              "  'scores': tensor([0.9939, 0.9925])},\n",
              " {'boxes': tensor([[ 85.6107,  85.8513, 162.8731, 279.5236],\n",
              "          [160.8502,  61.5694, 225.1740, 276.4303]], grad_fn=<StackBackward0>),\n",
              "  'labels': tensor([2, 3]),\n",
              "  'scores': tensor([0.9973, 0.9515])},\n",
              " {'boxes': tensor([[ 27.1274,   5.4286, 197.1422, 231.7677]], grad_fn=<StackBackward0>),\n",
              "  'labels': tensor([1]),\n",
              "  'scores': tensor([0.9312])},\n",
              " {'boxes': tensor([[  3.7197, 247.8615,  58.1924, 319.3000]], grad_fn=<StackBackward0>),\n",
              "  'labels': tensor([3]),\n",
              "  'scores': tensor([0.8059])},\n",
              " {'boxes': tensor([[ 78.9867,  64.2120, 217.2306, 237.6544]], grad_fn=<StackBackward0>),\n",
              "  'labels': tensor([1]),\n",
              "  'scores': tensor([0.9318])},\n",
              " {'boxes': tensor([[137.2188, 153.9799, 267.0954, 313.8381]], grad_fn=<StackBackward0>),\n",
              "  'labels': tensor([2]),\n",
              "  'scores': tensor([0.9765])},\n",
              " {'boxes': tensor([], size=(1, 0)),\n",
              "  'labels': tensor([]),\n",
              "  'scores': tensor([])},\n",
              " {'boxes': tensor([[144.1634,  49.2664, 259.5536, 271.4836],\n",
              "          [ 46.4425,   4.6094, 156.1039, 311.0587],\n",
              "          [132.6616, 100.5494, 174.3637, 236.4232]], grad_fn=<StackBackward0>),\n",
              "  'labels': tensor([3, 3, 3]),\n",
              "  'scores': tensor([0.9973, 0.9773, 0.5589])},\n",
              " {'boxes': tensor([[ 70.8878,  64.3830, 230.2723, 294.9669],\n",
              "          [263.4974, 110.6995, 297.5823, 175.6465]], grad_fn=<StackBackward0>),\n",
              "  'labels': tensor([2, 3]),\n",
              "  'scores': tensor([0.9581, 0.8994])},\n",
              " {'boxes': tensor([[ 95.7570, 131.6015, 182.8741, 261.0623],\n",
              "          [ 57.4863, 121.2908, 173.3777, 252.7001]], grad_fn=<StackBackward0>),\n",
              "  'labels': tensor([1, 3]),\n",
              "  'scores': tensor([0.9793, 0.8479])}]"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b = a[-3:]\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GK6FJdqABKsu",
        "outputId": "95f89a61-757f-4875-eaf3-403161b3c7e0"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'boxes': tensor([[144.1634,  49.2664, 259.5536, 271.4836],\n",
            "        [ 46.4425,   4.6094, 156.1039, 311.0587],\n",
            "        [132.6616, 100.5494, 174.3637, 236.4232]], grad_fn=<StackBackward0>), 'scores': tensor([0.9973, 0.9773, 0.5589]), 'labels': tensor([3, 3, 3])}, {'boxes': tensor([[ 70.8878,  64.3830, 230.2723, 294.9669],\n",
            "        [263.4974, 110.6995, 297.5823, 175.6465]], grad_fn=<StackBackward0>), 'scores': tensor([0.9581, 0.8994]), 'labels': tensor([2, 3])}, {'boxes': tensor([[ 95.7570, 131.6015, 182.8741, 261.0623],\n",
            "        [ 57.4863, 121.2908, 173.3777, 252.7001]], grad_fn=<StackBackward0>), 'scores': tensor([0.9793, 0.8479]), 'labels': tensor([1, 3])}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SP_1-Bt4Iluu",
        "outputId": "3988cde9-5a62-4fd3-f4ed-df6dcb79b4c6"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'boxes': tensor([[144.1634,  49.2664, 259.5536, 271.4836],\n",
              "          [ 46.4425,   4.6094, 156.1039, 311.0587],\n",
              "          [132.6616, 100.5494, 174.3637, 236.4232]], grad_fn=<StackBackward0>),\n",
              "  'labels': tensor([3, 3, 3]),\n",
              "  'scores': tensor([0.9973, 0.9773, 0.5589])},\n",
              " {'boxes': tensor([[ 70.8878,  64.3830, 230.2723, 294.9669],\n",
              "          [263.4974, 110.6995, 297.5823, 175.6465]], grad_fn=<StackBackward0>),\n",
              "  'labels': tensor([2, 3]),\n",
              "  'scores': tensor([0.9581, 0.8994])},\n",
              " {'boxes': tensor([[ 95.7570, 131.6015, 182.8741, 261.0623],\n",
              "          [ 57.4863, 121.2908, 173.3777, 252.7001]], grad_fn=<StackBackward0>),\n",
              "  'labels': tensor([1, 3]),\n",
              "  'scores': tensor([0.9793, 0.8479])}]"
            ]
          },
          "metadata": {},
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds = []\n",
        "for idx, ls_for_each_img in enumerate(b):\n",
        "  # print(idx, ls_for_each_img)\n",
        "  boxes = ls_for_each_img[\"boxes\"]\n",
        "  scores = ls_for_each_img[\"scores\"]\n",
        "  labels = ls_for_each_img[\"labels\"]\n",
        "  boxes = boxes.cpu().detach().numpy()\n",
        "  scores = scores.cpu().detach().numpy()\n",
        "  labels = labels.cpu().detach().numpy()\n",
        "  for single_detection in zip(boxes, scores, labels):\n",
        "    print(single_detection)\n",
        "    # print(np.shape(single_detection[1]))\n",
        "    # if np.shape(single_detection[1])[1] == 0:\n",
        "    #   continue\n",
        "    preds.append([idx, single_detection[2], single_detection[1], single_detection[0][0], single_detection[0][1], single_detection[0][2], single_detection[0][3]])\n",
        "\n",
        "print(preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyQOOTPI-eAi",
        "outputId": "17735eb7-2f65-4551-b40f-f6f9d91ee542"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(array([144.16339,  49.26644, 259.55356, 271.4836 ], dtype=float32), 0.99726784, 3)\n",
            "(array([ 46.44245  ,   4.6094055, 156.10388  , 311.05875  ], dtype=float32), 0.97725004, 3)\n",
            "(array([132.66158, 100.5494 , 174.36372, 236.42319], dtype=float32), 0.55890584, 3)\n",
            "(array([ 70.887825,  64.38302 , 230.27231 , 294.96695 ], dtype=float32), 0.9580607, 2)\n",
            "(array([263.49738, 110.69947, 297.58234, 175.64648], dtype=float32), 0.8993826, 3)\n",
            "(array([ 95.75699, 131.60147, 182.87408, 261.06232], dtype=float32), 0.9792961, 1)\n",
            "(array([ 57.48625 , 121.290794, 173.3777  , 252.70007 ], dtype=float32), 0.8478921, 3)\n",
            "[[0, 3, 0.99726784, 144.16339, 49.26644, 259.55356, 271.4836], [0, 3, 0.97725004, 46.44245, 4.6094055, 156.10388, 311.05875], [0, 3, 0.55890584, 132.66158, 100.5494, 174.36372, 236.42319], [1, 2, 0.9580607, 70.887825, 64.38302, 230.27231, 294.96695], [1, 3, 0.8993826, 263.49738, 110.69947, 297.58234, 175.64648], [2, 1, 0.9792961, 95.75699, 131.60147, 182.87408, 261.06232], [2, 3, 0.8478921, 57.48625, 121.290794, 173.3777, 252.70007]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_target[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CunMD8Dd-37j",
        "outputId": "9f236163-c344-439f-86df-e68a09d949ca"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'boxes': tensor([[ 75,  18, 171, 148]], dtype=torch.int32),\n",
              " 'labels': tensor([1])}"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "true_boxes = []\n",
        "for idx, target_for_each_img in enumerate(temp_target):\n",
        "  # print(target_for_each_img)\n",
        "  boxes = target_for_each_img[\"boxes\"]\n",
        "  labels = target_for_each_img[\"labels\"]\n",
        "  boxes = boxes.cpu().detach().numpy()\n",
        "  labels = labels.cpu().detach().numpy()\n",
        "  for single_detection in zip(boxes, labels):\n",
        "    # print(single_detection)\n",
        "    true_boxes.append([idx, single_detection[1], 1, single_detection[0][0], single_detection[0][1], single_detection[0][2], single_detection[0][3]])\n",
        "print(true_boxes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqQKbbUPDK0c",
        "outputId": "f640d6d0-8f6e-406c-8f80-44c2349436fa"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0, 1, 1, 75, 18, 171, 148], [1, 1, 1, 91, 118, 165, 216], [2, 1, 1, 127, 121, 248, 270], [3, 1, 1, 194, 211, 276, 305], [4, 1, 1, 140, 74, 237, 205], [5, 1, 1, 72, 26, 166, 152], [6, 2, 1, 162, 140, 176, 181], [6, 3, 1, 256, 84, 291, 196], [6, 3, 1, 147, 49, 198, 152], [6, 3, 1, 136, 136, 157, 172], [7, 2, 1, 91, 85, 161, 278], [7, 3, 1, 164, 58, 221, 273], [8, 1, 1, 16, 1, 217, 262], [9, 3, 1, 0, 246, 62, 317], [10, 1, 1, 39, 69, 202, 300], [11, 2, 1, 128, 151, 263, 316], [12, 2, 1, 139, 167, 161, 223], [13, 2, 1, 164, 132, 216, 251], [13, 3, 1, 156, 58, 250, 268], [13, 3, 1, 43, 0, 145, 312], [13, 3, 1, 134, 85, 169, 231], [14, 2, 1, 63, 61, 246, 296], [14, 3, 1, 263, 110, 298, 177], [15, 1, 1, 88, 126, 185, 275]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "temp_trash = ([165.23242, 146.69205, 291.3508 , 304.7331 ], 0.9991553, 1)\n",
        "print(temp_trash)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgUNHnyDENtR",
        "outputId": "23d4611e-9574-4440-e77a-13e232e6b43f"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([165.23242, 146.69205, 291.3508, 304.7331], 0.9991553, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Dt6B2_blENrI"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "9GOYhqV8ENo3"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YIzBNHgKENml"
      },
      "execution_count": 104,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "final model evaluation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "92f4577f6b7f49efb1cfdcd0ac3fa970": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6ef1152dcf4744cba8ccf671a2cef7b4",
              "IPY_MODEL_ebf7a8e183fa4805b119ac1f3b5d60b5",
              "IPY_MODEL_764902e58891449d883c2404b1cfe685"
            ],
            "layout": "IPY_MODEL_69bf78f941734de8ac1a37f30800b891"
          }
        },
        "6ef1152dcf4744cba8ccf671a2cef7b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2789d3100f9a46d09d137136a511a353",
            "placeholder": "",
            "style": "IPY_MODEL_ff5040abe94042e98192596c5dd98ff6",
            "value": "  0%"
          }
        },
        "ebf7a8e183fa4805b119ac1f3b5d60b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32f916a41c274058a64c269f8ef1bb6f",
            "max": 80,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c80c000f7fd84bcea296d8d2825b7030",
            "value": 0
          }
        },
        "764902e58891449d883c2404b1cfe685": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d7453d7e5aa4aed915f794314a0bf86",
            "placeholder": "",
            "style": "IPY_MODEL_f6ba6c536221476f8ed98431cd292608",
            "value": " 0/80 [00:00&lt;?, ?it/s]"
          }
        },
        "69bf78f941734de8ac1a37f30800b891": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2789d3100f9a46d09d137136a511a353": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff5040abe94042e98192596c5dd98ff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32f916a41c274058a64c269f8ef1bb6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c80c000f7fd84bcea296d8d2825b7030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5d7453d7e5aa4aed915f794314a0bf86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6ba6c536221476f8ed98431cd292608": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "773b6776f3db4a6b978753faa435fbc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ffcac1f3f796444a93d3683b08babe5e",
              "IPY_MODEL_1778dc3b43494d1182b8446652f56458",
              "IPY_MODEL_ca198789fa584aa3a252efa42b831ff5"
            ],
            "layout": "IPY_MODEL_8bad99ec6f90496898568a5ab4920685"
          }
        },
        "ffcac1f3f796444a93d3683b08babe5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2f3577c08a14a24b64f19dea7281f47",
            "placeholder": "",
            "style": "IPY_MODEL_2a789d8763874f06937928cdfbeaef17",
            "value": "100%"
          }
        },
        "1778dc3b43494d1182b8446652f56458": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37428a272dc54301b155dba261808713",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b450178b4d874b80b3de0e5214af8589",
            "value": 20
          }
        },
        "ca198789fa584aa3a252efa42b831ff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9e1f1e887d1471398e43d938a3cdfa5",
            "placeholder": "",
            "style": "IPY_MODEL_85142b00eebf4e0cad35c703ebda97a5",
            "value": " 20/20 [00:23&lt;00:00,  1.15s/it]"
          }
        },
        "8bad99ec6f90496898568a5ab4920685": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2f3577c08a14a24b64f19dea7281f47": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a789d8763874f06937928cdfbeaef17": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "37428a272dc54301b155dba261808713": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b450178b4d874b80b3de0e5214af8589": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e9e1f1e887d1471398e43d938a3cdfa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85142b00eebf4e0cad35c703ebda97a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "669712cd1c694c308a6cb0180fbaf094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83bdb133af6f437786ed0c661d5ee71b",
              "IPY_MODEL_5bcb88b1f6684343b0df7348c80d5509",
              "IPY_MODEL_222bd5bd3928497faf7fda38566d1948"
            ],
            "layout": "IPY_MODEL_d8d032db23a34d7f80fb8c894a99daac"
          }
        },
        "83bdb133af6f437786ed0c661d5ee71b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0fea0b75510c49e88c8b9168f1bcb33e",
            "placeholder": "",
            "style": "IPY_MODEL_c8f63152d9cf4cd1bc9dd85f7fed416d",
            "value": "100%"
          }
        },
        "5bcb88b1f6684343b0df7348c80d5509": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10c5cf27b83a4814a7094067670532f5",
            "max": 20,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c048b99e65db46ef92ea2adc8b3fd65a",
            "value": 20
          }
        },
        "222bd5bd3928497faf7fda38566d1948": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65db7d7fc71d4102a783d2803a8f5cb7",
            "placeholder": "",
            "style": "IPY_MODEL_cc8c27343f30462694ac19ac0f529cda",
            "value": " 20/20 [00:23&lt;00:00,  1.19s/it]"
          }
        },
        "d8d032db23a34d7f80fb8c894a99daac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fea0b75510c49e88c8b9168f1bcb33e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8f63152d9cf4cd1bc9dd85f7fed416d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10c5cf27b83a4814a7094067670532f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c048b99e65db46ef92ea2adc8b3fd65a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "65db7d7fc71d4102a783d2803a8f5cb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc8c27343f30462694ac19ac0f529cda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}